{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aaricis/AI-Expert-Roadmap/blob/main/HW12/ML2022__hw12_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 思路\n",
        "助教发布作业时，Colab的Python版本为3.9，笔者做作业的当下Colab的Python版本已升级为3.11。原先的依赖版本已经不能在Python 3.11环境使用，笔者升级依赖版本，重新配置了运行环境。因此，环境实际Baseline已与助教当时提供的不同，助教Baseline不具有参考意义。\n",
        "\n",
        "出于学习的目的，下面仅根据助教的提示，实现各个Baseline对应的trick，重点关注score的相对提升，不关注具体的值。\n",
        "\n"
      ],
      "metadata": {
        "id": "nHrYGs7sWqCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Baseline (Score: [0, 110])\n",
        "Your final reward is : 75.46\n",
        "\n",
        "跑通Sample Code\n",
        "\n",
        "```python\n",
        "NUM_BATCH = 1000\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-SPp3TgYW7cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medium Baseline (Score: [110, 180])\n",
        "\n",
        "Your final reward is : 156.89\n",
        "\n",
        "\n",
        "- 设置NUM_BATCH;\n",
        "\n",
        "```python\n",
        "NUM_BATCH = 500\n",
        "```\n",
        "\n",
        "- 将reward计算方式改为discounted reward;\n",
        "\n",
        "删除原本使用的Immediate Reward:\n",
        "\n",
        "\n",
        "```python\n",
        "# rewards.append(reward)\n",
        "```\n",
        "\n",
        "改成Discounted Reward:\n",
        "\n",
        "\n",
        "```python\n",
        "if done:\n",
        "    final_rewards.append(reward)\n",
        "    total_rewards.append(total_reward)\n",
        "\n",
        "    T = len(seq_rewards)  # total steps\n",
        "    gamma = 0.99\n",
        "    discounted_rewards = [0] * T  # initialize the rewards\n",
        "\n",
        "    # calculated backwards\n",
        "    cumulative = 0\n",
        "    for t in reversed(range(T)):\n",
        "        cumulative = seq_rewards[t] + gamma * cumulative\n",
        "        discounted_rewards[t] = cumulative\n",
        "\n",
        "    rewards += discounted_rewards\n",
        "    break\n",
        "```"
      ],
      "metadata": {
        "id": "o_I1ngGCmC5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Strong Baseline (Score: [180, 275])\n",
        "\n",
        "Your final reward is : 232.88\n",
        "\n",
        "### Actor-Critic\n",
        "\n",
        "Actor-Critic算法是强化学习中的一种混合框架，结合了策略梯度（Actor）和值函数估计（Critic）的优势，既能直接优化策略，又能通过值函数减少训练方差。\n",
        "\n",
        "- Actor（策略网络）：负责生成动作的策略函数$\\pi(a|s;\\theta)$，直接控制智能体行为，告诉智能体当前状态下应做什么动作。\n",
        "  - 输入：状态$s$；\n",
        "  - 输出：动作概率分布（离散）或动作均值/方差（连续）。\n",
        "- Critic（值函数网络）：估计当前策略的“未来回报”，指导 Actor 改善。\n",
        "  - 输入：状态$s$；\n",
        "  - 输出：标量价值估计。\n",
        "- 协同机制：Actor根据Critic的评价调整策略，Crtic通过TD误差优化价值估计。\n",
        "\n",
        "### 训练流程\n",
        "1. 采样交互：\n",
        "  \n",
        "  与环境交互，得到$(s_t, a_t, r_t, s_{t+1})$;\n",
        "2. 计算TD误差或Advantage:\n",
        "  \n",
        "  使用Critic来评估当前策略的表现，两种方式：\n",
        "  - TD-error（Temporal Difference）：\n",
        "    $$\n",
        "    \\delta = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
        "    $$\n",
        "  - Advantage（优势函数）：\n",
        "    $$\n",
        "    A(s_t, a_t) = Q(s_t, a_t) - V(s_t) \\approx \\delta\n",
        "    $$\n",
        "3. 更新Critic（值函数）：\n",
        "  \n",
        "  使用TD误差训练Critic的参数，最小化：\n",
        "  $$\n",
        "  L_{critic} = (r_t + \\gamma V(s_{t+1}) - V(s_t))^2\n",
        "  $$\n",
        "4. 更新Actor（策略网络）：\n",
        "  \n",
        "  使用策略梯度，最大化优势：\n",
        "  $$\n",
        "  L_{actor} = -log\\pi(a_t|s_t;\\theta) \\cdot A(s_t, a_t)\n",
        "  $$\n",
        "\n",
        "### 代码实现\n",
        "#### Actor\n",
        "```pyhton\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_size=8, action_size=4, hidden_size=64):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(state_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, action_size),\n",
        "        nn.Softmax(dim=-1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state):\n",
        "    # Returing probability of each action\n",
        "    return self.fc(state)\n",
        "\n",
        "```\n",
        "\n",
        "#### Critic\n",
        "```python\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_size=8, hidden_size=64, drop_prob=0.3):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(state_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=drop_prob),\n",
        "        nn.Linear(hidden_size, hidden_size // 2),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=drop_prob),\n",
        "        nn.Linear(hidden_size // 2, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state):\n",
        "    # critic: evaluates being in the state s_t\n",
        "    return self.fc(state)\n",
        "```\n",
        "#### Actor Critic Agent\n",
        "```python\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from argparse import Namespace\n",
        "\n",
        "class ActorCriticAgent():\n",
        "  def __init__(self, actor, critic, **kwargs):\n",
        "    # Configuration parameters\n",
        "    self.config = Namespace(**kwargs)\n",
        "\n",
        "    # Actor-Critic Network\n",
        "    self.actor = actor\n",
        "    self.critic = critic\n",
        "    self.optimizer_actor = getattr(optim, self.config.optimizer)(self.actor.parameters(), lr=self.config.learning_rate)\n",
        "    self.optimizer_critic = getattr(optim, self.config.optimizer)(self.critic.parameters(), lr=self.config.learning_rate)\n",
        "    self.loss_fn = nn.SmoothL1Loss()\n",
        "\n",
        "    # Step and update frequency\n",
        "    self.step_t = 0\n",
        "    self.update_freq = self.config.update_freq\n",
        "\n",
        "    # Records\n",
        "    self.loss_values = []\n",
        "\n",
        "    self.empty()\n",
        "\n",
        "  def step(self, log_probs, rewards, state_values, next_state_values, dones):\n",
        "    self.step_t = (self.step_t + 1) % self.update_freq\n",
        "\n",
        "    # Append the experiences\n",
        "    self.rewards += rewards\n",
        "    self.log_probs += log_probs\n",
        "    self.state_values += state_values\n",
        "    self.next_state_values += next_state_values\n",
        "    self.dones += dones\n",
        "\n",
        "    # Update Network\n",
        "    if self.step_t == 0:\n",
        "      self.learn(\n",
        "          torch.stack(self.log_probs),  # log probabilities\n",
        "          torch.tensor(self.rewards, dtype=torch.float32),  # discounted cumulative rewards\n",
        "          torch.tensor(self.state_values, requires_grad=True),  # state_values\n",
        "          torch.tensor(self.next_state_values, requires_grad=True), # next_state_values\n",
        "          torch.tensor(self.dones, dtype=torch.float32) # dones\n",
        "      )\n",
        "\n",
        "      # Empty the experiences\n",
        "      self.empty()\n",
        "\n",
        "  def empty(self):\n",
        "      \"\"\"\n",
        "      Empty the experience list\n",
        "      \"\"\"\n",
        "      self.rewards = []\n",
        "      self.log_probs = []\n",
        "      self.state_values = []\n",
        "      self.next_state_values = []\n",
        "      self.dones = []\n",
        "\n",
        "  def learn(self, log_probs, rewards, state_values, next_state_values, dones):\n",
        "    \"\"\"\n",
        "    Update value parameters using given experience list.\n",
        "\n",
        "    Arguments:\n",
        "      log_probs (torch.Tensor): log probabilities\n",
        "      rewards (torch.Tensor): discounted cumulative rewards\n",
        "      state_values (torch.Tensor): predicted current state_values\n",
        "      next_state_values (torch.Tensor): predicted next state_values\n",
        "      dones (torch.Tensor): dones\n",
        "\n",
        "    \"\"\"\n",
        "    state_values = state_values.squeeze()\n",
        "    next_state_values = next_state_values.squeeze()\n",
        "\n",
        "    gamma = 0.99\n",
        "    advantages = rewards + gamma * next_state_values * (1 - dones) - state_values\n",
        "\n",
        "    # Normalization\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-9)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss_actor = (-log_probs * advantages).sum()\n",
        "    loss_critic = self.loss_fn(state_values, rewards)\n",
        "    self.loss_values.append(loss_actor.detach().item() + loss_critic.detach().item())\n",
        "\n",
        "    # Backpropagation\n",
        "    self.optimizer_actor.zero_grad()\n",
        "    self.optimizer_critic.zero_grad()\n",
        "    loss_actor.backward()\n",
        "    loss_critic.backward()\n",
        "    self.optimizer_actor.step()\n",
        "    self.optimizer_critic.step()\n",
        "\n",
        "  def sample(self, state):\n",
        "    \"\"\"\n",
        "    Return action, log_prob, state_value for given state.\n",
        "\n",
        "    Arguments:\n",
        "      state(array_like): current state\n",
        "    \"\"\"\n",
        "    action_prob = self.actor(torch.FloatTensor(state))\n",
        "    state_value = self.critic(torch.FloatTensor(state))\n",
        "\n",
        "    action_dist = Categorical(action_prob)\n",
        "    action = action_dist.sample()\n",
        "    log_prob = action_dist.log_prob(action)\n",
        "    return action.item(), log_prob, state_value\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "81N3xspxoCmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boss Baseline (Score: [275, inf))\n",
        "Your final reward is : 291.98\n",
        "\n",
        "### Deep Q-Network (DQN)\n",
        "LunarLander任务的动作空间是离散的，并且DQN非常适合离散动作的环境，因此实作中选择DQN算法训练LunarLander任务。\n",
        "\n",
        "Deep Q-Network(DQN)是深度强化学习(DRL)中的一种经典算法，由DeepMind在2013年提出。核心思想是用神经网络近似Q值函数，解决了传统Q-Learning在高维状态空间下的局限性。\n",
        "\n",
        "1. Q-Learning简述\n",
        "\n",
        "Q-Learning的目标是学习状态-动作值函数$Q(s, a)$，表示在状态$s$采取动作$a$后的预期回报。更新公式如下：\n",
        "$$\n",
        "Q(s_t, a_t) ← Q(s_t, a_t) + \\alpha (r_t + \\gamma \\mathop{\\max}\\limits_{a^{\\prime}} Q(s_{t+1}, a^{\\prime}) - Q(s_t, a_t))\n",
        "$$\n",
        "\n",
        "2. Deep Q-Network（DQN）核心思想\n",
        "\n",
        "DQN使用一个神经网络来逼近Q函数：\n",
        "$$\n",
        "Q(s, a; \\theta) \\approx Q^{*}(s, a)\n",
        "$$\n",
        "- 输入：当前状态$s$；\n",
        "- 输出：每个可能动作的Q值$Q(s, a)$；\n",
        "\n",
        "- 目标：最小化Bellman残差：\n",
        "$$\n",
        "L(\\theta) = (r + \\gamma \\mathop{\\max}\\limits_{a^{\\prime}} Q(s^{\\prime}, a^{\\prime}; \\theta^{-}) - Q(s, a; \\theta))^2\n",
        "$$\n",
        "其中$\\theta^{-}$是目标网络的参数，是$\\theta$的一个延迟副本。\n",
        "\n",
        "3. 核心机制\n",
        "DQN的核心机制有：\n",
        "- 经验回放（Experience Replay）\n",
        "- 目标网络（Target Network）\n",
        "- $\\epsilon - Greedy$策略\n",
        "\n",
        "详情参考原始论文[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)\n",
        "\n",
        "### 代码实现\n",
        "DQN是经典的深度强化学习算法，有标准的库可供调用，不必重复造轮子。实作调用Stable-Baselines3库的DQN实现。\n",
        "#### 导入库\n",
        "\n",
        "```python\n",
        "!pip install stable-baselines3[extra]\n",
        "```\n",
        "#### 训练\n",
        "```python\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.utils import get_schedule_fn\n",
        "\n",
        "# 定义学习率调度（从 1e-3 线性衰减到 1e-5）\n",
        "lr_schedule = get_schedule_fn(\n",
        "    lambda progress: 1e-3 * (1 - progress) + 1e-5 * progress\n",
        ")\n",
        "\n",
        "model = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    learning_rate=lr_schedule(0.0),\n",
        "    buffer_size=500_000,\n",
        "    learning_starts=10_000,\n",
        "    batch_size=64,\n",
        "    tau=1.0,\n",
        "    gamma=0.99,\n",
        "    train_freq=4,\n",
        "    target_update_interval=5_000,\n",
        "    exploration_fraction=0.2,        # 20% 步骤用于探索\n",
        "    exploration_final_eps=0.05,\n",
        "    policy_kwargs=dict(net_arch=[256, 256]),\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# 训练前评估初始随机策略\n",
        "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"初始平均奖励: {mean_reward:.2f}\")\n",
        "\n",
        "# 训练\n",
        "model.learn(total_timesteps=2_000_000)\n",
        "\n",
        "# 评估\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
        "print(f\"✅ Evaluation over 20 episodes: mean reward = {mean_reward:.2f} ± {std_reward:.2f}\")\n",
        "\n",
        "# 保存模型\n",
        "model.save(\"dqn_lunarlander_best\")\n",
        "\n",
        "```\n",
        "#### 测试\n",
        "\n",
        "```python\n",
        "# For DQN\n",
        "fix(env, seed)\n",
        "\n",
        "del model # remove to demonstrate saving and loading\n",
        "# 加载模型\n",
        "model = DQN.load(\"dqn_lunarlander_best\")\n",
        "\n",
        "NUM_OF_TEST = 5 # Do not revise this !!!\n",
        "test_total_reward = []\n",
        "action_list = []\n",
        "\n",
        "\n",
        "for i in range(NUM_OF_TEST):\n",
        "  actions = []\n",
        "  state, _ = env.reset()\n",
        "\n",
        "  img = plt.imshow(env.render())\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "      action, _states = model.predict(state, deterministic=True)\n",
        "      actions.append(action.item())\n",
        "      # state, reward, done, _ = env.step(action)\n",
        "      state, reward, terminated, truncated, info = env.step(action)\n",
        "      done = terminated or truncated\n",
        "\n",
        "      total_reward += reward\n",
        "\n",
        "      img.set_data(env.render())\n",
        "      display.display(plt.gcf())\n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "  print(total_reward)\n",
        "  test_total_reward.append(total_reward)\n",
        "\n",
        "  action_list.append(actions) # save the result of testing\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IQwXoNX88ANY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp30SB4bxeQb"
      },
      "source": [
        "# **Homework 12 - Reinforcement Learning**\n",
        "\n",
        "If you have any problem, e-mail us at mlta-2023-spring@googlegroups.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXsnCWPtWSNk"
      },
      "source": [
        "## Preliminary work\n",
        "\n",
        "First, we need to install all necessary packages.\n",
        "One of them, gym, builded by OpenAI, is a toolkit for developing Reinforcement Learning algorithm. Other packages are for visualization in colab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Python 3.11\n",
        "# Step 0: 清理已有的 gym 和 box2d 避免冲突\n",
        "!pip uninstall -y gym gymnasium box2d box2d-py swig_box2d\n",
        "\n",
        "# Step 1: 安装 C++ 依赖\n",
        "!apt update -y\n",
        "!apt install -y swig cmake libbox2d-dev\n",
        "\n",
        "# Step 2: 安装 Python 包\n",
        "!pip install gymnasium[box2d] pygame opencv-python-headless\n",
        "!pip install PyVirtualDisplay\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMD8d56ewjBh",
        "outputId": "3977472e-2d5e-4523-8fe5-954f31ef00aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gym 0.25.2\n",
            "Uninstalling gym-0.25.2:\n",
            "  Successfully uninstalled gym-0.25.2\n",
            "Found existing installation: gymnasium 1.1.1\n",
            "Uninstalling gymnasium-1.1.1:\n",
            "  Successfully uninstalled gymnasium-1.1.1\n",
            "\u001b[33mWARNING: Skipping box2d as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping box2d-py as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping swig_box2d as it is not installed.\u001b[0m\u001b[33m\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,724 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,290 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,468 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,979 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,622 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,553 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,999 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,740 kB]\n",
            "Fetched 32.0 MB in 7s (4,823 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "The following additional packages will be installed:\n",
            "  libbox2d2 swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  libbox2d-dev libbox2d2 swig swig4.0\n",
            "0 upgraded, 4 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 1,259 kB of archives.\n",
            "After this operation, 6,073 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbox2d2 amd64 2.4.1-2ubuntu1 [102 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libbox2d-dev amd64 2.4.1-2ubuntu1 [41.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,259 kB in 0s (7,839 kB/s)\n",
            "Selecting previously unselected package libbox2d2:amd64.\n",
            "(Reading database ... 126109 files and directories currently installed.)\n",
            "Preparing to unpack .../libbox2d2_2.4.1-2ubuntu1_amd64.deb ...\n",
            "Unpacking libbox2d2:amd64 (2.4.1-2ubuntu1) ...\n",
            "Selecting previously unselected package libbox2d-dev:amd64.\n",
            "Preparing to unpack .../libbox2d-dev_2.4.1-2ubuntu1_amd64.deb ...\n",
            "Unpacking libbox2d-dev:amd64 (2.4.1-2ubuntu1) ...\n",
            "Selecting previously unselected package swig4.0.\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up libbox2d2:amd64 (2.4.1-2ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Setting up libbox2d-dev:amd64 (2.4.1-2ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Collecting gymnasium[box2d]\n",
            "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2351178 sha256=a8eca8a8b4052d85a7c8da78dcaa95132c7cf0d808c053a36a14142ae763c162\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py, gymnasium\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gym<=0.25.2, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed box2d-py-2.3.5 gymnasium-1.1.1 swig-4.3.1\n",
            "Collecting PyVirtualDisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: PyVirtualDisplay\n",
            "Successfully installed PyVirtualDisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 测试代码（运行环境是否成功）\n",
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "obs, info = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "env.close()\n",
        "\n",
        "print(\"✅ LunarLander-v2 ran successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK-fyBKDyPnP",
        "outputId": "63e32521-a72f-4882-8fb9-0b1c5c95d373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LunarLander-v2 ran successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e2bScpnkVbv"
      },
      "source": [
        "# 助教原本安装环境\n",
        "# !apt update\n",
        "# !apt install python-opengl xvfb -y\n",
        "# !pip install -q swig\n",
        "# !pip install box2d==2.3.2 gym[box2d]==0.25.2 box2d-py pyvirtualdisplay tqdm numpy==1.22.4\n",
        "# !pip install box2d==2.3.2 box2d-kengz\n",
        "# !pip freeze > requirements.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_-i3cdoYsks"
      },
      "source": [
        "\n",
        "Next, set up virtual display，and import all necessaary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2nREINDLiw"
      },
      "source": [
        "%%capture\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaEJ8BUCpN9P"
      },
      "source": [
        "# Warning ! Do not revise random seed !!!\n",
        "# Your submission on JudgeBoi will not reproduce your result !!!\n",
        "Make your HW result to be reproducible.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV9i8i2YkRbO"
      },
      "source": [
        "seed = 2023 # Do not change this\n",
        "def fix(env, seed):\n",
        "  # env.seed(seed)\n",
        "  env.reset(seed=seed)\n",
        "  env.action_space.seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if torch.cuda.is_available():\n",
        "      torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He0XDx6bzjgC"
      },
      "source": [
        "Last, call gym and build an [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_4-xJcbBt09"
      },
      "source": [
        "%%capture\n",
        "# import gym\n",
        "import random\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "fix(env, seed) # fix the environment Do not revise this !!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkVvTrvWZ5H"
      },
      "source": [
        "## What Lunar Lander？\n",
        "\n",
        "“LunarLander-v2”is to simulate the situation when the craft lands on the surface of the moon.\n",
        "\n",
        "This task is to enable the craft to land \"safely\" at the pad between the two yellow flags.\n",
        "> Landing pad is always at coordinates (0,0).\n",
        "> Coordinates are the first two numbers in state vector.\n",
        "\n",
        "![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n",
        "\n",
        "\"LunarLander-v2\" actually includes \"Agent\" and \"Environment\".\n",
        "\n",
        "In this homework, we will utilize the function `step()` to control the action of \"Agent\".\n",
        "\n",
        "Then `step()` will return the observation/state and reward given by the \"Environment\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIbp82sljvAt"
      },
      "source": [
        "### Observation / State\n",
        "\n",
        "First, we can take a look at what an Observation / State looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsXZra3N9R5T",
        "outputId": "d5b43e36-6633-4e53-c9d0-c66765196eba"
      },
      "source": [
        "print(env.observation_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdfoThbAQ49"
      },
      "source": [
        "\n",
        "`Box(8,)`means that observation is an 8-dim vector\n",
        "### Action\n",
        "\n",
        "Actions can be taken by looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1k4dIrBAaKi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f1b29ae-aff6-4422-f295-61e5adfe8c89"
      },
      "source": [
        "print(env.action_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dejXT6PHBrPn"
      },
      "source": [
        "`Discrete(4)` implies that there are four kinds of actions can be taken by agent.\n",
        "- 0 implies the agent will not take any actions\n",
        "- 2 implies the agent will accelerate downward\n",
        "- 1, 3 implies the agent will accelerate left and right\n",
        "\n",
        "Next, we will try to make the agent interact with the environment.\n",
        "Before taking any actions, we recommend to call `reset()` function to reset the environment. Also, this function will return the initial state of the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi4OmrmZgnWA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f41866bf-f407-4dee-a17c-74ad013c1626"
      },
      "source": [
        "initial_state = env.reset()\n",
        "print(initial_state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([-0.00552921,  1.4196991 , -0.5600674 ,  0.3901667 ,  0.00641378,\n",
            "        0.12686351,  0.        ,  0.        ], dtype=float32), {})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBx0mEqqgxJ9"
      },
      "source": [
        "Then, we try to get a random action from the agent's action space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxkOEXRKgizt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49cde552-5a37-4852-a954-1d7e32f7eba2"
      },
      "source": [
        "random_action = env.action_space.sample()\n",
        "print(random_action)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mns-bO01g0-J"
      },
      "source": [
        "More, we can utilize `step()` to make agent act according to the randomly-selected `random_action`.\n",
        "The `step()` function will return four values:\n",
        "- observation / state\n",
        "- reward\n",
        "- done (True/ False)\n",
        "- Other information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_WViSxGgIk9"
      },
      "source": [
        "# observation, reward, done, info = env.step(random_action)\n",
        "observation, reward, terminated, truncated, info = env.step(random_action)\n",
        "done = terminated or truncated"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK7r126kuCNp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9a1741-cf3b-4e6b-db5f-e81e7e35d1e6"
      },
      "source": [
        "print(done)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdS8vOihxhc"
      },
      "source": [
        "### Reward\n",
        "\n",
        "\n",
        "> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxQNs77hi0_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be509c35-0911-4ac5-f34d-4ccaa04d8a9a"
      },
      "source": [
        "print(reward)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.8266717815513107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhqp6D-XgHpe"
      },
      "source": [
        "### Random Agent\n",
        "In the end, before we start training, we can see whether a random agent can successfully land the moon or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3G0bxoccelv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "0be4a422-92b7-4076-d5f6-f62727760309"
      },
      "source": [
        "env.reset()\n",
        "\n",
        "# img = plt.imshow(env.render(mode='rgb_array'))\n",
        "img = plt.imshow(env.render())\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    # observation, reward, done, _ = env.step(action)\n",
        "\n",
        "    img.set_data(env.render())\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5paWqo7tWL2"
      },
      "source": [
        "## Policy Gradient\n",
        "Now, we can build a simple policy network. The network will return one of action in the action space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8tdmeD-tZew"
      },
      "source": [
        "class PolicyGradientNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(8, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 4)\n",
        "\n",
        "    def forward(self, state):\n",
        "        hid = torch.tanh(self.fc1(state))\n",
        "        hid = torch.tanh(hid)\n",
        "        return F.softmax(self.fc3(hid), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynbqJrhIFTC3"
      },
      "source": [
        "Then, we need to build a simple agent. The agent will acts according to the output of the policy network above. There are a few things can be done by agent:\n",
        "- `learn()`：update the policy network from log probabilities and rewards.\n",
        "- `sample()`：After receiving observation from the environment, utilize policy network to tell which action to take. The return values of this function includes action and log probabilities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZo-IxJx286z"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "class PolicyGradientAgent():\n",
        "\n",
        "    def __init__(self, network):\n",
        "        self.network = network\n",
        "        self.optimizer = optim.SGD(self.network.parameters(), lr=0.002)\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network(state)\n",
        "    def learn(self, log_probs, rewards):\n",
        "        loss = (-log_probs * rewards).sum() # You don't need to revise this to pass simple baseline (but you can)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def sample(self, state):\n",
        "        action_prob = self.network(torch.FloatTensor(state))\n",
        "        action_dist = Categorical(action_prob)\n",
        "        action = action_dist.sample()\n",
        "        log_prob = action_dist.log_prob(action)\n",
        "        return action.item(), log_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehPlnTKyRZf9"
      },
      "source": [
        "Lastly, build a network and agent to start training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfJIvML-RYjL"
      },
      "source": [
        "network = PolicyGradientNetwork()\n",
        "agent = PolicyGradientAgent(network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouv23glgf5Qt"
      },
      "source": [
        "## Training Agent\n",
        "\n",
        "Now let's start to train our agent.\n",
        "Through taking all the interactions between agent and environment as training data, the policy network can learn from all these attempts,"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.network.train()  # Switch network into training mode\n",
        "EPISODE_PER_BATCH = 5  # update the  agent every 5 episode\n",
        "NUM_BATCH = 1000 # 100        # totally update the agent for 500 time\n",
        "\n",
        "avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "for batch in prg_bar:\n",
        "\n",
        "    log_probs, rewards = [], []\n",
        "    total_rewards, final_rewards = [], []\n",
        "\n",
        "    # collect trajectory\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "\n",
        "        state, _ = env.reset()\n",
        "        total_reward, total_step = 0, 0\n",
        "        seq_rewards = []\n",
        "        while True:\n",
        "\n",
        "            action, log_prob = agent.sample(state) # at, log(at|st)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
        "            # seq_rewards.append(reward)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "            rewards.append(reward) # change here\n",
        "            # ! IMPORTANT !\n",
        "            # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
        "            #                                                         rewards :     r1, r2 ,r3 ......\n",
        "            # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
        "            #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
        "            # boss : implement Actor-Critic\n",
        "            if done:\n",
        "                final_rewards.append(reward)\n",
        "                total_rewards.append(total_reward)\n",
        "\n",
        "                break\n",
        "\n",
        "    print(f\"rewards looks like \", np.shape(rewards))\n",
        "    #print(f\"log_probs looks like \", np.shape(log_probs))\n",
        "    # record training process\n",
        "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "    avg_total_rewards.append(avg_total_reward)\n",
        "    avg_final_rewards.append(avg_final_reward)\n",
        "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "\n",
        "    # update agent\n",
        "    # rewards = np.concatenate(rewards, axis=0)\n",
        "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward\n",
        "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
        "    print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
        "    print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())"
      ],
      "metadata": {
        "id": "ufaX2M0AnCHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg5rxBBaf38_"
      },
      "source": [
        "# Medium Baseline\n",
        "agent.network.train()  # Switch network into training mode\n",
        "EPISODE_PER_BATCH = 5  # update the  agent every 5 episode\n",
        "NUM_BATCH = 500        # totally update the agent for 500 time\n",
        "\n",
        "avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "for batch in prg_bar:\n",
        "\n",
        "    log_probs, rewards = [], []\n",
        "    total_rewards, final_rewards = [], []\n",
        "\n",
        "    # collect trajectory\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "\n",
        "        state, _ = env.reset()\n",
        "        total_reward, total_step = 0, 0\n",
        "        seq_rewards = []\n",
        "        while True:\n",
        "\n",
        "            action, log_prob = agent.sample(state) # at, log(at|st)\n",
        "            # next_state, reward, done, _ = env.step(action)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
        "            seq_rewards.append(reward)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "            # rewards.append(reward) # change here\n",
        "            # ! IMPORTANT !\n",
        "            # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
        "            #                                                         rewards :     r1, r2 ,r3 ......\n",
        "            # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
        "            #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
        "            # boss : implement Actor-Critic\n",
        "            if done:\n",
        "                final_rewards.append(reward)\n",
        "                total_rewards.append(total_reward)\n",
        "\n",
        "                T = len(seq_rewards)  # total steps\n",
        "                gamma = 0.99\n",
        "                discounted_rewards = [0] * T  # initialize the rewards\n",
        "\n",
        "                # calculated backwards\n",
        "                cumulative = 0\n",
        "                for t in reversed(range(T)):\n",
        "                    cumulative = seq_rewards[t] + gamma * cumulative\n",
        "                    discounted_rewards[t] = cumulative\n",
        "\n",
        "                rewards += discounted_rewards\n",
        "                break\n",
        "\n",
        "    print(f\"rewards looks like \", np.shape(rewards))\n",
        "    #print(f\"log_probs looks like \", np.shape(log_probs))\n",
        "    # record training process\n",
        "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "    avg_total_rewards.append(avg_total_reward)\n",
        "    avg_final_rewards.append(avg_final_reward)\n",
        "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "\n",
        "    # update agent\n",
        "    # rewards = np.concatenate(rewards, axis=0)\n",
        "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # normalize the reward\n",
        "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
        "    print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
        "    print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNb_tuFYhKVK"
      },
      "source": [
        "### Training Result\n",
        "During the training process, we recorded `avg_total_reward`, which represents the average total reward of episodes before updating the policy network.\n",
        "\n",
        "Theoretically, if the agent becomes better, the `avg_total_reward` will increase.\n",
        "The visualization of the training process is shown below:  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZYOI8H10SHN"
      },
      "source": [
        "plt.plot(avg_total_rewards)\n",
        "plt.title(\"Total Rewards\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV5jj4dThz0Y"
      },
      "source": [
        "In addition, `avg_final_reward` represents average final rewards of episodes. To be specific, final rewards is the last reward received in one episode, indicating whether the craft lands successfully or not.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txDZ5vlGWz5w"
      },
      "source": [
        "plt.plot(avg_final_rewards)\n",
        "plt.title(\"Final Rewards\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Actor-Critic"
      ],
      "metadata": {
        "id": "XbPcqs87Z9uF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor"
      ],
      "metadata": {
        "id": "KQsD_f1hcxWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self, state_size=8, action_size=4, hidden_size=64):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(state_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, action_size),\n",
        "        nn.Softmax(dim=-1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state):\n",
        "    # Returing probability of each action\n",
        "    return self.fc(state)\n"
      ],
      "metadata": {
        "id": "fzrCSSlwaxz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Critic"
      ],
      "metadata": {
        "id": "QR58GqMSc0Sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self, state_size=8, hidden_size=64, drop_prob=0.3):\n",
        "    super().__init__()\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(state_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=drop_prob),\n",
        "        nn.Linear(hidden_size, hidden_size // 2),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=drop_prob),\n",
        "        nn.Linear(hidden_size // 2, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state):\n",
        "    # critic: evaluates being in the state s_t\n",
        "    return self.fc(state)"
      ],
      "metadata": {
        "id": "fz7NhC1GcwPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actor Critic Agent"
      ],
      "metadata": {
        "id": "5ggZ4X8reKKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "from argparse import Namespace\n",
        "\n",
        "class ActorCriticAgent():\n",
        "  def __init__(self, actor, critic, **kwargs):\n",
        "    # Configuration parameters\n",
        "    self.config = Namespace(**kwargs)\n",
        "\n",
        "    # Actor-Critic Network\n",
        "    self.actor = actor\n",
        "    self.critic = critic\n",
        "    self.optimizer_actor = getattr(optim, self.config.optimizer)(self.actor.parameters(), lr=self.config.learning_rate)\n",
        "    self.optimizer_critic = getattr(optim, self.config.optimizer)(self.critic.parameters(), lr=self.config.learning_rate)\n",
        "    self.loss_fn = nn.SmoothL1Loss()\n",
        "\n",
        "    # Step and update frequency\n",
        "    self.step_t = 0\n",
        "    self.update_freq = self.config.update_freq\n",
        "\n",
        "    # Records\n",
        "    self.loss_values = []\n",
        "\n",
        "    self.empty()\n",
        "\n",
        "  def step(self, log_probs, rewards, state_values, next_state_values, dones):\n",
        "    self.step_t = (self.step_t + 1) % self.update_freq\n",
        "\n",
        "    # Append the experiences\n",
        "    self.rewards += rewards\n",
        "    self.log_probs += log_probs\n",
        "    self.state_values += state_values\n",
        "    self.next_state_values += next_state_values\n",
        "    self.dones += dones\n",
        "\n",
        "    # Update Network\n",
        "    if self.step_t == 0:\n",
        "      self.learn(\n",
        "          torch.stack(self.log_probs),  # log probabilities\n",
        "          torch.tensor(self.rewards, dtype=torch.float32),  # discounted cumulative rewards\n",
        "          torch.tensor(self.state_values, requires_grad=True),  # state_values\n",
        "          torch.tensor(self.next_state_values, requires_grad=True), # next_state_values\n",
        "          torch.tensor(self.dones, dtype=torch.float32) # dones\n",
        "      )\n",
        "\n",
        "      # Empty the experiences\n",
        "      self.empty()\n",
        "\n",
        "  def empty(self):\n",
        "      \"\"\"\n",
        "      Empty the experience list\n",
        "      \"\"\"\n",
        "      self.rewards = []\n",
        "      self.log_probs = []\n",
        "      self.state_values = []\n",
        "      self.next_state_values = []\n",
        "      self.dones = []\n",
        "\n",
        "  def learn(self, log_probs, rewards, state_values, next_state_values, dones):\n",
        "    \"\"\"\n",
        "    Update value parameters using given experience list.\n",
        "\n",
        "    Arguments:\n",
        "      log_probs (torch.Tensor): log probabilities\n",
        "      rewards (torch.Tensor): discounted cumulative rewards\n",
        "      state_values (torch.Tensor): predicted current state_values\n",
        "      next_state_values (torch.Tensor): predicted next state_values\n",
        "      dones (torch.Tensor): dones\n",
        "\n",
        "    \"\"\"\n",
        "    state_values = state_values.squeeze()\n",
        "    next_state_values = next_state_values.squeeze()\n",
        "\n",
        "    gamma = 0.99\n",
        "    advantages = rewards + gamma * next_state_values * (1 - dones) - state_values\n",
        "\n",
        "    # Normalization\n",
        "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-9)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss_actor = (-log_probs * advantages).sum()\n",
        "    loss_critic = self.loss_fn(state_values, rewards)\n",
        "    self.loss_values.append(loss_actor.detach().item() + loss_critic.detach().item())\n",
        "\n",
        "    # Backpropagation\n",
        "    self.optimizer_actor.zero_grad()\n",
        "    self.optimizer_critic.zero_grad()\n",
        "    loss_actor.backward()\n",
        "    loss_critic.backward()\n",
        "    self.optimizer_actor.step()\n",
        "    self.optimizer_critic.step()\n",
        "\n",
        "  def sample(self, state):\n",
        "    \"\"\"\n",
        "    Return action, log_prob, state_value for given state.\n",
        "\n",
        "    Arguments:\n",
        "      state(array_like): current state\n",
        "    \"\"\"\n",
        "    action_prob = self.actor(torch.FloatTensor(state))\n",
        "    state_value = self.critic(torch.FloatTensor(state))\n",
        "\n",
        "    action_dist = Categorical(action_prob)\n",
        "    action = action_dist.sample()\n",
        "    log_prob = action_dist.log_prob(action)\n",
        "    return action.item(), log_prob, state_value"
      ],
      "metadata": {
        "id": "ekHRYLwceP8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Actor_config = Namespace(\n",
        "    state_size = 8,\n",
        "    action_size = 4,\n",
        "    hidden_size = 64,\n",
        ")\n",
        "\n",
        "Critic_config = Namespace(\n",
        "    state_size = 8,\n",
        "    hidden_size = 64,\n",
        "    drop_prob = 0.3,\n",
        ")\n",
        "\n",
        "Agent_config = Namespace(\n",
        "    optimizer = 'Adam',\n",
        "    learning_rate = 5e-4,\n",
        "    update_freq = 5,\n",
        ")\n",
        "\n",
        "ActorCritic_config = Namespace(\n",
        "    n_episodes = 10000,\n",
        "    max_t = None,\n",
        "    window_size = 500,\n",
        "    gamma = 0.99,\n",
        ")\n",
        "\n",
        "Actor_config, Critic_config, Agent_config, ActorCritic_config"
      ],
      "metadata": {
        "id": "J3YqRvQMM8Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actor = Actor(**vars(Actor_config)) # **vars(...) 相当于把这个字典解包成关键字参数传入 Actor(...) 构造函数\n",
        "critic = Critic(**vars(Critic_config))\n",
        "agent = ActorCriticAgent(actor, critic, **vars(Agent_config))"
      ],
      "metadata": {
        "id": "iYqFC43CNFZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import deque\n",
        "\n",
        "def ActorCritic(n_episodes=2000, max_t=1000, window_size=100, gamma=0.99):\n",
        "  \"\"\"\n",
        "  A2C Algorithm.\n",
        "  Arguments:\n",
        "    n_episodes (int): maximum number of training episodes\n",
        "    max_t (int): maximum number of timesteps per episode(None: No limit)\n",
        "    window_size (int): maximum size to record observed total rewards\n",
        "    gamma (float): discount factor\n",
        "  \"\"\"\n",
        "  agent.actor.train() # Switch network into training mode\n",
        "  agent.critic.train()  # Switch network into training mode\n",
        "\n",
        "  total_rewards = []  # list containing total reward from each episode\n",
        "  final_rewards = []  # list containing final reward from each episode\n",
        "  rewards_window = deque(maxlen=window_size)  # last window_size rewards\n",
        "  best_reward = -np.inf\n",
        "\n",
        "  progress_bar = tqdm(range(1, n_episodes + 1))\n",
        "  for i_episode in progress_bar:\n",
        "    state, _ = env.reset() # Reset the environment\n",
        "    total_reward, reward = 0, 0 # Total reward / current reward\n",
        "    max_t = max_t if max_t else np.inf  # max_t is infinite if the value id None\n",
        "    total_step = 0  # Step of each episode\n",
        "\n",
        "    rewards = []\n",
        "    log_probs = []\n",
        "    state_values = []\n",
        "    next_state_values = []\n",
        "    dones = []\n",
        "\n",
        "    while True:\n",
        "      # Reach the max_t\n",
        "      if total_step == max_t:\n",
        "        break\n",
        "\n",
        "      # Sample an action and interact with the environment\n",
        "      action, log_prob, state_value = agent.sample(state) # action at, log(at|st)\n",
        "      next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "      done = terminated or truncated\n",
        "      _, _, next_state_value = agent.sample(next_state) # action at, log(at+1|st+1)\n",
        "\n",
        "      # Information about a state\n",
        "      next_state_values.append(next_state_value)\n",
        "      state_values.append(state_value)\n",
        "      rewards.append(reward)\n",
        "      dones.append(done)\n",
        "      log_probs.append(log_prob)\n",
        "\n",
        "      state = next_state\n",
        "      total_reward += reward\n",
        "      total_step += 1\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    \"\"\" Discounted cumulative rewards \"\"\"\n",
        "    discounted_rewards = []\n",
        "    running_sum = 0\n",
        "    for r in reversed(rewards):\n",
        "      running_sum = r + gamma * running_sum\n",
        "      discounted_rewards.insert(0, running_sum)\n",
        "    assert (len(rewards) == len(discounted_rewards)), \"Dimension does not match.\"\n",
        "    rewards = discounted_rewards\n",
        "\n",
        "    final_rewards.append(reward)  # save final reward in episode\n",
        "    total_rewards.append(total_reward)  # save total rewards in episode\n",
        "    rewards_window.append(total_reward) # save total rewards in episode\n",
        "\n",
        "    # Agent performs on step\n",
        "    agent.step(log_probs, rewards, state_values, next_state_values, dones)\n",
        "\n",
        "    # record training process\n",
        "    progress_bar.set_description(f\"Total: {total_reward: 4.1f}, Final: {reward: 4.1f}\")\n",
        "    # Information\n",
        "    print(f'Episode {i_episode}\\t Average: {window_size} Score: {np.mean(rewards_window): 4.1f} \\t TotalScore: {total_reward: 4.1f}{\" \"*5}', end='\\r')\n",
        "    if i_episode % window_size == 0:\n",
        "      print(f'Episode {i_episode} \\t Average: {window_size} Score: {np.mean(rewards_window): 4.1f}')\n",
        "\n",
        "    if total_reward > best_reward:\n",
        "      torch.save(agent.actor.state_dict(), f'actor.pt')\n",
        "      best_reward = total_reward\n",
        "  return total_rewards, final_rewards\n"
      ],
      "metadata": {
        "id": "nfVpzgHfQrDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_rewards, final_rewards = ActorCritic(**vars(ActorCritic_config))"
      ],
      "metadata": {
        "id": "a-cqXVEkwneo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(agent.loss_values)\n",
        "plt.title(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ACgO3THzw8is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(total_rewards)\n",
        "plt.title(\"Total Rewards\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I8G9sYIsxA-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(final_rewards)\n",
        "plt.title(\"Final Rewards\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J6RoLyLbxGLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Q-Network"
      ],
      "metadata": {
        "id": "X_722jY1hg_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3[extra]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPCbJCaLhoUS",
        "outputId": "d8b39756-68c6-401b-fdd6-d00e5d5a17d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (1.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (11.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3[extra])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stable-baselines3-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.utils import get_schedule_fn\n",
        "\n",
        "# 定义学习率调度（从 1e-3 线性衰减到 1e-5）\n",
        "lr_schedule = get_schedule_fn(\n",
        "    lambda progress: 1e-3 * (1 - progress) + 1e-5 * progress\n",
        ")\n",
        "\n",
        "model = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    learning_rate=lr_schedule(0.0),\n",
        "    buffer_size=500_000,\n",
        "    learning_starts=10_000,\n",
        "    batch_size=64,\n",
        "    tau=1.0,\n",
        "    gamma=0.99,\n",
        "    train_freq=4,\n",
        "    target_update_interval=5_000,\n",
        "    exploration_fraction=0.2,        # 20% 步骤用于探索\n",
        "    exploration_final_eps=0.05,\n",
        "    policy_kwargs=dict(net_arch=[256, 256]),\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# 训练前评估初始随机策略\n",
        "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
        "print(f\"初始平均奖励: {mean_reward:.2f}\")\n",
        "\n",
        "# 训练\n",
        "model.learn(total_timesteps=2_000_000)\n",
        "\n",
        "# 评估\n",
        "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
        "print(f\"✅ Evaluation over 20 episodes: mean reward = {mean_reward:.2f} ± {std_reward:.2f}\")\n",
        "\n",
        "# 保存模型\n",
        "model.save(\"dqn_lunarlander_best\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-F1OUgpkByC",
        "outputId": "719b4dbd-b38a-4d3d-cfee-54b7a9113f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.479    |\n",
            "|    n_updates        | 3479     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93       |\n",
            "|    ep_rew_mean      | -157     |\n",
            "|    exploration_rate | 0.942    |\n",
            "| time/               |          |\n",
            "|    episodes         | 256      |\n",
            "|    fps              | 892      |\n",
            "|    time_elapsed     | 27       |\n",
            "|    total_timesteps  | 24268    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.749    |\n",
            "|    n_updates        | 3566     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.3     |\n",
            "|    ep_rew_mean      | -155     |\n",
            "|    exploration_rate | 0.941    |\n",
            "| time/               |          |\n",
            "|    episodes         | 260      |\n",
            "|    fps              | 884      |\n",
            "|    time_elapsed     | 27       |\n",
            "|    total_timesteps  | 24746    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.17     |\n",
            "|    n_updates        | 3686     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.8     |\n",
            "|    ep_rew_mean      | -154     |\n",
            "|    exploration_rate | 0.94     |\n",
            "| time/               |          |\n",
            "|    episodes         | 264      |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 28       |\n",
            "|    total_timesteps  | 25060    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.53     |\n",
            "|    n_updates        | 3764     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.2     |\n",
            "|    ep_rew_mean      | -153     |\n",
            "|    exploration_rate | 0.94     |\n",
            "| time/               |          |\n",
            "|    episodes         | 268      |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 29       |\n",
            "|    total_timesteps  | 25418    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.318    |\n",
            "|    n_updates        | 3854     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.5     |\n",
            "|    ep_rew_mean      | -152     |\n",
            "|    exploration_rate | 0.939    |\n",
            "| time/               |          |\n",
            "|    episodes         | 272      |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 29       |\n",
            "|    total_timesteps  | 25775    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.62     |\n",
            "|    n_updates        | 3943     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.6     |\n",
            "|    ep_rew_mean      | -155     |\n",
            "|    exploration_rate | 0.938    |\n",
            "| time/               |          |\n",
            "|    episodes         | 276      |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 30       |\n",
            "|    total_timesteps  | 26170    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.67     |\n",
            "|    n_updates        | 4042     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.5     |\n",
            "|    ep_rew_mean      | -154     |\n",
            "|    exploration_rate | 0.937    |\n",
            "| time/               |          |\n",
            "|    episodes         | 280      |\n",
            "|    fps              | 862      |\n",
            "|    time_elapsed     | 30       |\n",
            "|    total_timesteps  | 26532    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.387    |\n",
            "|    n_updates        | 4132     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.4     |\n",
            "|    ep_rew_mean      | -151     |\n",
            "|    exploration_rate | 0.936    |\n",
            "| time/               |          |\n",
            "|    episodes         | 284      |\n",
            "|    fps              | 859      |\n",
            "|    time_elapsed     | 31       |\n",
            "|    total_timesteps  | 26890    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.82     |\n",
            "|    n_updates        | 4222     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.2     |\n",
            "|    ep_rew_mean      | -152     |\n",
            "|    exploration_rate | 0.935    |\n",
            "| time/               |          |\n",
            "|    episodes         | 288      |\n",
            "|    fps              | 854      |\n",
            "|    time_elapsed     | 31       |\n",
            "|    total_timesteps  | 27305    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.62     |\n",
            "|    n_updates        | 4326     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.7     |\n",
            "|    ep_rew_mean      | -153     |\n",
            "|    exploration_rate | 0.934    |\n",
            "| time/               |          |\n",
            "|    episodes         | 292      |\n",
            "|    fps              | 850      |\n",
            "|    time_elapsed     | 32       |\n",
            "|    total_timesteps  | 27738    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.33     |\n",
            "|    n_updates        | 4434     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97       |\n",
            "|    ep_rew_mean      | -152     |\n",
            "|    exploration_rate | 0.933    |\n",
            "| time/               |          |\n",
            "|    episodes         | 296      |\n",
            "|    fps              | 841      |\n",
            "|    time_elapsed     | 33       |\n",
            "|    total_timesteps  | 28112    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.41     |\n",
            "|    n_updates        | 4527     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.4     |\n",
            "|    ep_rew_mean      | -152     |\n",
            "|    exploration_rate | 0.932    |\n",
            "| time/               |          |\n",
            "|    episodes         | 300      |\n",
            "|    fps              | 834      |\n",
            "|    time_elapsed     | 34       |\n",
            "|    total_timesteps  | 28435    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.76     |\n",
            "|    n_updates        | 4608     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.5     |\n",
            "|    ep_rew_mean      | -149     |\n",
            "|    exploration_rate | 0.932    |\n",
            "| time/               |          |\n",
            "|    episodes         | 304      |\n",
            "|    fps              | 827      |\n",
            "|    time_elapsed     | 34       |\n",
            "|    total_timesteps  | 28748    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.83     |\n",
            "|    n_updates        | 4686     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.9     |\n",
            "|    ep_rew_mean      | -146     |\n",
            "|    exploration_rate | 0.931    |\n",
            "| time/               |          |\n",
            "|    episodes         | 308      |\n",
            "|    fps              | 820      |\n",
            "|    time_elapsed     | 35       |\n",
            "|    total_timesteps  | 29157    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.97     |\n",
            "|    n_updates        | 4789     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.1     |\n",
            "|    ep_rew_mean      | -142     |\n",
            "|    exploration_rate | 0.93     |\n",
            "| time/               |          |\n",
            "|    episodes         | 312      |\n",
            "|    fps              | 818      |\n",
            "|    time_elapsed     | 36       |\n",
            "|    total_timesteps  | 29518    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.88     |\n",
            "|    n_updates        | 4879     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.9     |\n",
            "|    ep_rew_mean      | -141     |\n",
            "|    exploration_rate | 0.929    |\n",
            "| time/               |          |\n",
            "|    episodes         | 316      |\n",
            "|    fps              | 816      |\n",
            "|    time_elapsed     | 36       |\n",
            "|    total_timesteps  | 29948    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 4986     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.4     |\n",
            "|    ep_rew_mean      | -139     |\n",
            "|    exploration_rate | 0.928    |\n",
            "| time/               |          |\n",
            "|    episodes         | 320      |\n",
            "|    fps              | 814      |\n",
            "|    time_elapsed     | 37       |\n",
            "|    total_timesteps  | 30284    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 5070     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.5     |\n",
            "|    ep_rew_mean      | -138     |\n",
            "|    exploration_rate | 0.927    |\n",
            "| time/               |          |\n",
            "|    episodes         | 324      |\n",
            "|    fps              | 812      |\n",
            "|    time_elapsed     | 37       |\n",
            "|    total_timesteps  | 30634    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.78     |\n",
            "|    n_updates        | 5158     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.1     |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.926    |\n",
            "| time/               |          |\n",
            "|    episodes         | 328      |\n",
            "|    fps              | 810      |\n",
            "|    time_elapsed     | 38       |\n",
            "|    total_timesteps  | 30992    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.64     |\n",
            "|    n_updates        | 5247     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.9     |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.925    |\n",
            "| time/               |          |\n",
            "|    episodes         | 332      |\n",
            "|    fps              | 808      |\n",
            "|    time_elapsed     | 38       |\n",
            "|    total_timesteps  | 31379    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.41     |\n",
            "|    n_updates        | 5344     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.9     |\n",
            "|    ep_rew_mean      | -132     |\n",
            "|    exploration_rate | 0.925    |\n",
            "| time/               |          |\n",
            "|    episodes         | 336      |\n",
            "|    fps              | 805      |\n",
            "|    time_elapsed     | 39       |\n",
            "|    total_timesteps  | 31747    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.204    |\n",
            "|    n_updates        | 5436     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.4     |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.924    |\n",
            "| time/               |          |\n",
            "|    episodes         | 340      |\n",
            "|    fps              | 803      |\n",
            "|    time_elapsed     | 39       |\n",
            "|    total_timesteps  | 32100    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.89     |\n",
            "|    n_updates        | 5524     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.1     |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.923    |\n",
            "| time/               |          |\n",
            "|    episodes         | 344      |\n",
            "|    fps              | 800      |\n",
            "|    time_elapsed     | 40       |\n",
            "|    total_timesteps  | 32451    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.519    |\n",
            "|    n_updates        | 5612     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.8     |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.922    |\n",
            "| time/               |          |\n",
            "|    episodes         | 348      |\n",
            "|    fps              | 798      |\n",
            "|    time_elapsed     | 41       |\n",
            "|    total_timesteps  | 32886    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.97     |\n",
            "|    n_updates        | 5721     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.6     |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.921    |\n",
            "| time/               |          |\n",
            "|    episodes         | 352      |\n",
            "|    fps              | 795      |\n",
            "|    time_elapsed     | 41       |\n",
            "|    total_timesteps  | 33278    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.396    |\n",
            "|    n_updates        | 5819     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.6     |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.92     |\n",
            "| time/               |          |\n",
            "|    episodes         | 356      |\n",
            "|    fps              | 792      |\n",
            "|    time_elapsed     | 42       |\n",
            "|    total_timesteps  | 33626    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.207    |\n",
            "|    n_updates        | 5906     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.8     |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.919    |\n",
            "| time/               |          |\n",
            "|    episodes         | 360      |\n",
            "|    fps              | 789      |\n",
            "|    time_elapsed     | 43       |\n",
            "|    total_timesteps  | 34024    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.53     |\n",
            "|    n_updates        | 6005     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.9     |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.918    |\n",
            "| time/               |          |\n",
            "|    episodes         | 364      |\n",
            "|    fps              | 788      |\n",
            "|    time_elapsed     | 43       |\n",
            "|    total_timesteps  | 34349    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.75     |\n",
            "|    n_updates        | 6087     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.2     |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.917    |\n",
            "| time/               |          |\n",
            "|    episodes         | 368      |\n",
            "|    fps              | 785      |\n",
            "|    time_elapsed     | 44       |\n",
            "|    total_timesteps  | 34740    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.256    |\n",
            "|    n_updates        | 6184     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.7     |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.917    |\n",
            "| time/               |          |\n",
            "|    episodes         | 372      |\n",
            "|    fps              | 783      |\n",
            "|    time_elapsed     | 44       |\n",
            "|    total_timesteps  | 35040    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.53     |\n",
            "|    n_updates        | 6259     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.3     |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.916    |\n",
            "| time/               |          |\n",
            "|    episodes         | 376      |\n",
            "|    fps              | 781      |\n",
            "|    time_elapsed     | 45       |\n",
            "|    total_timesteps  | 35398    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.275    |\n",
            "|    n_updates        | 6349     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.6     |\n",
            "|    ep_rew_mean      | -130     |\n",
            "|    exploration_rate | 0.915    |\n",
            "| time/               |          |\n",
            "|    episodes         | 380      |\n",
            "|    fps              | 775      |\n",
            "|    time_elapsed     | 46       |\n",
            "|    total_timesteps  | 35794    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.97     |\n",
            "|    n_updates        | 6448     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.7     |\n",
            "|    ep_rew_mean      | -129     |\n",
            "|    exploration_rate | 0.914    |\n",
            "| time/               |          |\n",
            "|    episodes         | 384      |\n",
            "|    fps              | 771      |\n",
            "|    time_elapsed     | 46       |\n",
            "|    total_timesteps  | 36157    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.98     |\n",
            "|    n_updates        | 6539     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.4     |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.913    |\n",
            "| time/               |          |\n",
            "|    episodes         | 388      |\n",
            "|    fps              | 765      |\n",
            "|    time_elapsed     | 47       |\n",
            "|    total_timesteps  | 36546    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.17     |\n",
            "|    n_updates        | 6636     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.5     |\n",
            "|    ep_rew_mean      | -129     |\n",
            "|    exploration_rate | 0.912    |\n",
            "| time/               |          |\n",
            "|    episodes         | 392      |\n",
            "|    fps              | 764      |\n",
            "|    time_elapsed     | 48       |\n",
            "|    total_timesteps  | 36889    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.86     |\n",
            "|    n_updates        | 6722     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 91.4     |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.912    |\n",
            "| time/               |          |\n",
            "|    episodes         | 396      |\n",
            "|    fps              | 763      |\n",
            "|    time_elapsed     | 48       |\n",
            "|    total_timesteps  | 37256    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.792    |\n",
            "|    n_updates        | 6813     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.5     |\n",
            "|    ep_rew_mean      | -130     |\n",
            "|    exploration_rate | 0.91     |\n",
            "| time/               |          |\n",
            "|    episodes         | 400      |\n",
            "|    fps              | 761      |\n",
            "|    time_elapsed     | 49       |\n",
            "|    total_timesteps  | 37690    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.95     |\n",
            "|    n_updates        | 6922     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.9     |\n",
            "|    ep_rew_mean      | -130     |\n",
            "|    exploration_rate | 0.91     |\n",
            "| time/               |          |\n",
            "|    episodes         | 404      |\n",
            "|    fps              | 760      |\n",
            "|    time_elapsed     | 50       |\n",
            "|    total_timesteps  | 38035    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.79     |\n",
            "|    n_updates        | 7008     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 92.8     |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.909    |\n",
            "| time/               |          |\n",
            "|    episodes         | 408      |\n",
            "|    fps              | 759      |\n",
            "|    time_elapsed     | 50       |\n",
            "|    total_timesteps  | 38437    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.02     |\n",
            "|    n_updates        | 7109     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.3     |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.908    |\n",
            "| time/               |          |\n",
            "|    episodes         | 412      |\n",
            "|    fps              | 758      |\n",
            "|    time_elapsed     | 51       |\n",
            "|    total_timesteps  | 38847    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.987    |\n",
            "|    n_updates        | 7211     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93       |\n",
            "|    ep_rew_mean      | -132     |\n",
            "|    exploration_rate | 0.907    |\n",
            "| time/               |          |\n",
            "|    episodes         | 416      |\n",
            "|    fps              | 757      |\n",
            "|    time_elapsed     | 51       |\n",
            "|    total_timesteps  | 39253    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.197    |\n",
            "|    n_updates        | 7313     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 93.8     |\n",
            "|    ep_rew_mean      | -132     |\n",
            "|    exploration_rate | 0.906    |\n",
            "| time/               |          |\n",
            "|    episodes         | 420      |\n",
            "|    fps              | 755      |\n",
            "|    time_elapsed     | 52       |\n",
            "|    total_timesteps  | 39666    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.99     |\n",
            "|    n_updates        | 7416     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 94.6     |\n",
            "|    ep_rew_mean      | -131     |\n",
            "|    exploration_rate | 0.905    |\n",
            "| time/               |          |\n",
            "|    episodes         | 424      |\n",
            "|    fps              | 753      |\n",
            "|    time_elapsed     | 53       |\n",
            "|    total_timesteps  | 40096    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 7523     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.1     |\n",
            "|    ep_rew_mean      | -130     |\n",
            "|    exploration_rate | 0.904    |\n",
            "| time/               |          |\n",
            "|    episodes         | 428      |\n",
            "|    fps              | 751      |\n",
            "|    time_elapsed     | 53       |\n",
            "|    total_timesteps  | 40499    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.1      |\n",
            "|    n_updates        | 7624     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 95.5     |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.903    |\n",
            "| time/               |          |\n",
            "|    episodes         | 432      |\n",
            "|    fps              | 750      |\n",
            "|    time_elapsed     | 54       |\n",
            "|    total_timesteps  | 40928    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.877    |\n",
            "|    n_updates        | 7731     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.5     |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.902    |\n",
            "| time/               |          |\n",
            "|    episodes         | 436      |\n",
            "|    fps              | 748      |\n",
            "|    time_elapsed     | 55       |\n",
            "|    total_timesteps  | 41399    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.49     |\n",
            "|    n_updates        | 7849     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.2     |\n",
            "|    ep_rew_mean      | -130     |\n",
            "|    exploration_rate | 0.901    |\n",
            "| time/               |          |\n",
            "|    episodes         | 440      |\n",
            "|    fps              | 746      |\n",
            "|    time_elapsed     | 55       |\n",
            "|    total_timesteps  | 41722    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.97     |\n",
            "|    n_updates        | 7930     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.5     |\n",
            "|    ep_rew_mean      | -132     |\n",
            "|    exploration_rate | 0.9      |\n",
            "| time/               |          |\n",
            "|    episodes         | 444      |\n",
            "|    fps              | 745      |\n",
            "|    time_elapsed     | 56       |\n",
            "|    total_timesteps  | 42099    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.376    |\n",
            "|    n_updates        | 8024     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.7     |\n",
            "|    ep_rew_mean      | -133     |\n",
            "|    exploration_rate | 0.899    |\n",
            "| time/               |          |\n",
            "|    episodes         | 448      |\n",
            "|    fps              | 744      |\n",
            "|    time_elapsed     | 57       |\n",
            "|    total_timesteps  | 42552    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.04     |\n",
            "|    n_updates        | 8137     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.4     |\n",
            "|    ep_rew_mean      | -134     |\n",
            "|    exploration_rate | 0.898    |\n",
            "| time/               |          |\n",
            "|    episodes         | 452      |\n",
            "|    fps              | 743      |\n",
            "|    time_elapsed     | 57       |\n",
            "|    total_timesteps  | 42921    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.63     |\n",
            "|    n_updates        | 8230     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.5     |\n",
            "|    ep_rew_mean      | -130     |\n",
            "|    exploration_rate | 0.897    |\n",
            "| time/               |          |\n",
            "|    episodes         | 456      |\n",
            "|    fps              | 738      |\n",
            "|    time_elapsed     | 58       |\n",
            "|    total_timesteps  | 43274    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.127    |\n",
            "|    n_updates        | 8318     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 96.2     |\n",
            "|    ep_rew_mean      | -130     |\n",
            "|    exploration_rate | 0.896    |\n",
            "| time/               |          |\n",
            "|    episodes         | 460      |\n",
            "|    fps              | 735      |\n",
            "|    time_elapsed     | 59       |\n",
            "|    total_timesteps  | 43644    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.51     |\n",
            "|    n_updates        | 8410     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.4     |\n",
            "|    ep_rew_mean      | -129     |\n",
            "|    exploration_rate | 0.895    |\n",
            "| time/               |          |\n",
            "|    episodes         | 464      |\n",
            "|    fps              | 730      |\n",
            "|    time_elapsed     | 60       |\n",
            "|    total_timesteps  | 44091    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.839    |\n",
            "|    n_updates        | 8522     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.3     |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.894    |\n",
            "| time/               |          |\n",
            "|    episodes         | 468      |\n",
            "|    fps              | 729      |\n",
            "|    time_elapsed     | 61       |\n",
            "|    total_timesteps  | 44569    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.77     |\n",
            "|    n_updates        | 8642     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.7     |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.893    |\n",
            "| time/               |          |\n",
            "|    episodes         | 472      |\n",
            "|    fps              | 727      |\n",
            "|    time_elapsed     | 61       |\n",
            "|    total_timesteps  | 44913    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.328    |\n",
            "|    n_updates        | 8728     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.9     |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.892    |\n",
            "| time/               |          |\n",
            "|    episodes         | 476      |\n",
            "|    fps              | 726      |\n",
            "|    time_elapsed     | 62       |\n",
            "|    total_timesteps  | 45291    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.15     |\n",
            "|    n_updates        | 8822     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.4     |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.891    |\n",
            "| time/               |          |\n",
            "|    episodes         | 480      |\n",
            "|    fps              | 725      |\n",
            "|    time_elapsed     | 63       |\n",
            "|    total_timesteps  | 45736    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.32     |\n",
            "|    n_updates        | 8933     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.5     |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.89     |\n",
            "| time/               |          |\n",
            "|    episodes         | 484      |\n",
            "|    fps              | 723      |\n",
            "|    time_elapsed     | 63       |\n",
            "|    total_timesteps  | 46112    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.712    |\n",
            "|    n_updates        | 9027     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.889    |\n",
            "| time/               |          |\n",
            "|    episodes         | 488      |\n",
            "|    fps              | 723      |\n",
            "|    time_elapsed     | 64       |\n",
            "|    total_timesteps  | 46553    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.367    |\n",
            "|    n_updates        | 9138     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.888    |\n",
            "| time/               |          |\n",
            "|    episodes         | 492      |\n",
            "|    fps              | 722      |\n",
            "|    time_elapsed     | 65       |\n",
            "|    total_timesteps  | 47020    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.92     |\n",
            "|    n_updates        | 9254     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.887    |\n",
            "| time/               |          |\n",
            "|    episodes         | 496      |\n",
            "|    fps              | 721      |\n",
            "|    time_elapsed     | 65       |\n",
            "|    total_timesteps  | 47455    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.166    |\n",
            "|    n_updates        | 9363     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.886    |\n",
            "| time/               |          |\n",
            "|    episodes         | 500      |\n",
            "|    fps              | 720      |\n",
            "|    time_elapsed     | 66       |\n",
            "|    total_timesteps  | 47850    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.44     |\n",
            "|    n_updates        | 9462     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.886    |\n",
            "| time/               |          |\n",
            "|    episodes         | 504      |\n",
            "|    fps              | 719      |\n",
            "|    time_elapsed     | 66       |\n",
            "|    total_timesteps  | 48183    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.718    |\n",
            "|    n_updates        | 9545     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.884    |\n",
            "| time/               |          |\n",
            "|    episodes         | 508      |\n",
            "|    fps              | 719      |\n",
            "|    time_elapsed     | 67       |\n",
            "|    total_timesteps  | 48651    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.441    |\n",
            "|    n_updates        | 9662     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.884    |\n",
            "| time/               |          |\n",
            "|    episodes         | 512      |\n",
            "|    fps              | 718      |\n",
            "|    time_elapsed     | 68       |\n",
            "|    total_timesteps  | 48971    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.81     |\n",
            "|    n_updates        | 9742     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.883    |\n",
            "| time/               |          |\n",
            "|    episodes         | 516      |\n",
            "|    fps              | 716      |\n",
            "|    time_elapsed     | 68       |\n",
            "|    total_timesteps  | 49387    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.27     |\n",
            "|    n_updates        | 9846     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -114     |\n",
            "|    exploration_rate | 0.882    |\n",
            "| time/               |          |\n",
            "|    episodes         | 520      |\n",
            "|    fps              | 715      |\n",
            "|    time_elapsed     | 69       |\n",
            "|    total_timesteps  | 49802    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.9      |\n",
            "|    n_updates        | 9950     |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.881    |\n",
            "| time/               |          |\n",
            "|    episodes         | 524      |\n",
            "|    fps              | 714      |\n",
            "|    time_elapsed     | 70       |\n",
            "|    total_timesteps  | 50153    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.387    |\n",
            "|    n_updates        | 10038    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.88     |\n",
            "| time/               |          |\n",
            "|    episodes         | 528      |\n",
            "|    fps              | 712      |\n",
            "|    time_elapsed     | 70       |\n",
            "|    total_timesteps  | 50540    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.869    |\n",
            "|    n_updates        | 10134    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.879    |\n",
            "| time/               |          |\n",
            "|    episodes         | 532      |\n",
            "|    fps              | 709      |\n",
            "|    time_elapsed     | 71       |\n",
            "|    total_timesteps  | 50972    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.89     |\n",
            "|    n_updates        | 10242    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99       |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.878    |\n",
            "| time/               |          |\n",
            "|    episodes         | 536      |\n",
            "|    fps              | 706      |\n",
            "|    time_elapsed     | 72       |\n",
            "|    total_timesteps  | 51301    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.211    |\n",
            "|    n_updates        | 10325    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.5     |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.877    |\n",
            "| time/               |          |\n",
            "|    episodes         | 540      |\n",
            "|    fps              | 704      |\n",
            "|    time_elapsed     | 73       |\n",
            "|    total_timesteps  | 51669    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.634    |\n",
            "|    n_updates        | 10417    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.2     |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.876    |\n",
            "| time/               |          |\n",
            "|    episodes         | 544      |\n",
            "|    fps              | 703      |\n",
            "|    time_elapsed     | 73       |\n",
            "|    total_timesteps  | 52018    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.202    |\n",
            "|    n_updates        | 10504    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.7     |\n",
            "|    ep_rew_mean      | -114     |\n",
            "|    exploration_rate | 0.876    |\n",
            "| time/               |          |\n",
            "|    episodes         | 548      |\n",
            "|    fps              | 700      |\n",
            "|    time_elapsed     | 74       |\n",
            "|    total_timesteps  | 52421    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.65     |\n",
            "|    n_updates        | 10605    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.4     |\n",
            "|    ep_rew_mean      | -114     |\n",
            "|    exploration_rate | 0.875    |\n",
            "| time/               |          |\n",
            "|    episodes         | 552      |\n",
            "|    fps              | 699      |\n",
            "|    time_elapsed     | 75       |\n",
            "|    total_timesteps  | 52759    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.88     |\n",
            "|    n_updates        | 10689    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.7     |\n",
            "|    ep_rew_mean      | -113     |\n",
            "|    exploration_rate | 0.874    |\n",
            "| time/               |          |\n",
            "|    episodes         | 556      |\n",
            "|    fps              | 699      |\n",
            "|    time_elapsed     | 75       |\n",
            "|    total_timesteps  | 53147    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.3      |\n",
            "|    n_updates        | 10786    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.9     |\n",
            "|    ep_rew_mean      | -113     |\n",
            "|    exploration_rate | 0.873    |\n",
            "| time/               |          |\n",
            "|    episodes         | 560      |\n",
            "|    fps              | 698      |\n",
            "|    time_elapsed     | 76       |\n",
            "|    total_timesteps  | 53537    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.77     |\n",
            "|    n_updates        | 10884    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.4     |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.872    |\n",
            "| time/               |          |\n",
            "|    episodes         | 564      |\n",
            "|    fps              | 698      |\n",
            "|    time_elapsed     | 77       |\n",
            "|    total_timesteps  | 53928    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 10981    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.2     |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.871    |\n",
            "| time/               |          |\n",
            "|    episodes         | 568      |\n",
            "|    fps              | 698      |\n",
            "|    time_elapsed     | 77       |\n",
            "|    total_timesteps  | 54289    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 11072    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98       |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.87     |\n",
            "| time/               |          |\n",
            "|    episodes         | 572      |\n",
            "|    fps              | 695      |\n",
            "|    time_elapsed     | 78       |\n",
            "|    total_timesteps  | 54714    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.13     |\n",
            "|    n_updates        | 11178    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.5     |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.869    |\n",
            "| time/               |          |\n",
            "|    episodes         | 576      |\n",
            "|    fps              | 693      |\n",
            "|    time_elapsed     | 79       |\n",
            "|    total_timesteps  | 55138    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.372    |\n",
            "|    n_updates        | 11284    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.9     |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.868    |\n",
            "| time/               |          |\n",
            "|    episodes         | 580      |\n",
            "|    fps              | 690      |\n",
            "|    time_elapsed     | 80       |\n",
            "|    total_timesteps  | 55624    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.591    |\n",
            "|    n_updates        | 11405    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.1     |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.867    |\n",
            "| time/               |          |\n",
            "|    episodes         | 584      |\n",
            "|    fps              | 690      |\n",
            "|    time_elapsed     | 81       |\n",
            "|    total_timesteps  | 56020    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.842    |\n",
            "|    n_updates        | 11504    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99       |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.866    |\n",
            "| time/               |          |\n",
            "|    episodes         | 588      |\n",
            "|    fps              | 689      |\n",
            "|    time_elapsed     | 81       |\n",
            "|    total_timesteps  | 56458    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.971    |\n",
            "|    n_updates        | 11614    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.5     |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.865    |\n",
            "| time/               |          |\n",
            "|    episodes         | 592      |\n",
            "|    fps              | 689      |\n",
            "|    time_elapsed     | 82       |\n",
            "|    total_timesteps  | 56865    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.537    |\n",
            "|    n_updates        | 11716    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.6     |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.864    |\n",
            "| time/               |          |\n",
            "|    episodes         | 596      |\n",
            "|    fps              | 688      |\n",
            "|    time_elapsed     | 83       |\n",
            "|    total_timesteps  | 57218    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.2      |\n",
            "|    n_updates        | 11804    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.5     |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.863    |\n",
            "| time/               |          |\n",
            "|    episodes         | 600      |\n",
            "|    fps              | 686      |\n",
            "|    time_elapsed     | 83       |\n",
            "|    total_timesteps  | 57597    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.63     |\n",
            "|    n_updates        | 11899    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.5     |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.862    |\n",
            "| time/               |          |\n",
            "|    episodes         | 604      |\n",
            "|    fps              | 683      |\n",
            "|    time_elapsed     | 84       |\n",
            "|    total_timesteps  | 58029    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.62     |\n",
            "|    n_updates        | 12007    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.3     |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.861    |\n",
            "| time/               |          |\n",
            "|    episodes         | 608      |\n",
            "|    fps              | 681      |\n",
            "|    time_elapsed     | 85       |\n",
            "|    total_timesteps  | 58386    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.15     |\n",
            "|    n_updates        | 12096    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.9     |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.86     |\n",
            "| time/               |          |\n",
            "|    episodes         | 612      |\n",
            "|    fps              | 681      |\n",
            "|    time_elapsed     | 86       |\n",
            "|    total_timesteps  | 58862    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.12     |\n",
            "|    n_updates        | 12215    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.1     |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.859    |\n",
            "| time/               |          |\n",
            "|    episodes         | 616      |\n",
            "|    fps              | 680      |\n",
            "|    time_elapsed     | 86       |\n",
            "|    total_timesteps  | 59198    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.5      |\n",
            "|    n_updates        | 12299    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.6     |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.859    |\n",
            "| time/               |          |\n",
            "|    episodes         | 620      |\n",
            "|    fps              | 680      |\n",
            "|    time_elapsed     | 87       |\n",
            "|    total_timesteps  | 59561    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.247    |\n",
            "|    n_updates        | 12390    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.9     |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.858    |\n",
            "| time/               |          |\n",
            "|    episodes         | 624      |\n",
            "|    fps              | 679      |\n",
            "|    time_elapsed     | 88       |\n",
            "|    total_timesteps  | 59943    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.756    |\n",
            "|    n_updates        | 12485    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.2     |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.857    |\n",
            "| time/               |          |\n",
            "|    episodes         | 628      |\n",
            "|    fps              | 675      |\n",
            "|    time_elapsed     | 89       |\n",
            "|    total_timesteps  | 60359    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.62     |\n",
            "|    n_updates        | 12589    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.8     |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.856    |\n",
            "| time/               |          |\n",
            "|    episodes         | 632      |\n",
            "|    fps              | 672      |\n",
            "|    time_elapsed     | 90       |\n",
            "|    total_timesteps  | 60751    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.44     |\n",
            "|    n_updates        | 12687    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.9     |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.855    |\n",
            "| time/               |          |\n",
            "|    episodes         | 636      |\n",
            "|    fps              | 669      |\n",
            "|    time_elapsed     | 91       |\n",
            "|    total_timesteps  | 61192    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.725    |\n",
            "|    n_updates        | 12797    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.853    |\n",
            "| time/               |          |\n",
            "|    episodes         | 640      |\n",
            "|    fps              | 665      |\n",
            "|    time_elapsed     | 92       |\n",
            "|    total_timesteps  | 61727    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.67     |\n",
            "|    n_updates        | 12931    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.853    |\n",
            "| time/               |          |\n",
            "|    episodes         | 644      |\n",
            "|    fps              | 665      |\n",
            "|    time_elapsed     | 93       |\n",
            "|    total_timesteps  | 62059    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.255    |\n",
            "|    n_updates        | 13014    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.852    |\n",
            "| time/               |          |\n",
            "|    episodes         | 648      |\n",
            "|    fps              | 665      |\n",
            "|    time_elapsed     | 93       |\n",
            "|    total_timesteps  | 62513    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.723    |\n",
            "|    n_updates        | 13128    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -112     |\n",
            "|    exploration_rate | 0.851    |\n",
            "| time/               |          |\n",
            "|    episodes         | 652      |\n",
            "|    fps              | 664      |\n",
            "|    time_elapsed     | 94       |\n",
            "|    total_timesteps  | 62870    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 8.03     |\n",
            "|    n_updates        | 13217    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -114     |\n",
            "|    exploration_rate | 0.85     |\n",
            "| time/               |          |\n",
            "|    episodes         | 656      |\n",
            "|    fps              | 664      |\n",
            "|    time_elapsed     | 95       |\n",
            "|    total_timesteps  | 63273    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 4.99     |\n",
            "|    n_updates        | 13318    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -113     |\n",
            "|    exploration_rate | 0.849    |\n",
            "| time/               |          |\n",
            "|    episodes         | 660      |\n",
            "|    fps              | 664      |\n",
            "|    time_elapsed     | 95       |\n",
            "|    total_timesteps  | 63638    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.645    |\n",
            "|    n_updates        | 13409    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -110     |\n",
            "|    exploration_rate | 0.848    |\n",
            "| time/               |          |\n",
            "|    episodes         | 664      |\n",
            "|    fps              | 662      |\n",
            "|    time_elapsed     | 96       |\n",
            "|    total_timesteps  | 64083    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.89     |\n",
            "|    n_updates        | 13520    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -110     |\n",
            "|    exploration_rate | 0.847    |\n",
            "| time/               |          |\n",
            "|    episodes         | 668      |\n",
            "|    fps              | 660      |\n",
            "|    time_elapsed     | 97       |\n",
            "|    total_timesteps  | 64475    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 13618    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -110     |\n",
            "|    exploration_rate | 0.846    |\n",
            "| time/               |          |\n",
            "|    episodes         | 672      |\n",
            "|    fps              | 658      |\n",
            "|    time_elapsed     | 98       |\n",
            "|    total_timesteps  | 64824    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.49     |\n",
            "|    n_updates        | 13705    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -108     |\n",
            "|    exploration_rate | 0.845    |\n",
            "| time/               |          |\n",
            "|    episodes         | 676      |\n",
            "|    fps              | 658      |\n",
            "|    time_elapsed     | 99       |\n",
            "|    total_timesteps  | 65250    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 13812    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -106     |\n",
            "|    exploration_rate | 0.844    |\n",
            "| time/               |          |\n",
            "|    episodes         | 680      |\n",
            "|    fps              | 658      |\n",
            "|    time_elapsed     | 99       |\n",
            "|    total_timesteps  | 65623    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.917    |\n",
            "|    n_updates        | 13905    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -104     |\n",
            "|    exploration_rate | 0.843    |\n",
            "| time/               |          |\n",
            "|    episodes         | 684      |\n",
            "|    fps              | 657      |\n",
            "|    time_elapsed     | 100      |\n",
            "|    total_timesteps  | 66126    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.789    |\n",
            "|    n_updates        | 14031    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.9     |\n",
            "|    ep_rew_mean      | -105     |\n",
            "|    exploration_rate | 0.842    |\n",
            "| time/               |          |\n",
            "|    episodes         | 688      |\n",
            "|    fps              | 657      |\n",
            "|    time_elapsed     | 101      |\n",
            "|    total_timesteps  | 66452    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.581    |\n",
            "|    n_updates        | 14112    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.8     |\n",
            "|    ep_rew_mean      | -105     |\n",
            "|    exploration_rate | 0.841    |\n",
            "| time/               |          |\n",
            "|    episodes         | 692      |\n",
            "|    fps              | 657      |\n",
            "|    time_elapsed     | 101      |\n",
            "|    total_timesteps  | 66840    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.39     |\n",
            "|    n_updates        | 14209    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -104     |\n",
            "|    exploration_rate | 0.84     |\n",
            "| time/               |          |\n",
            "|    episodes         | 696      |\n",
            "|    fps              | 656      |\n",
            "|    time_elapsed     | 102      |\n",
            "|    total_timesteps  | 67319    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.894    |\n",
            "|    n_updates        | 14329    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -103     |\n",
            "|    exploration_rate | 0.839    |\n",
            "| time/               |          |\n",
            "|    episodes         | 700      |\n",
            "|    fps              | 656      |\n",
            "|    time_elapsed     | 103      |\n",
            "|    total_timesteps  | 67686    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.278    |\n",
            "|    n_updates        | 14421    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -101     |\n",
            "|    exploration_rate | 0.838    |\n",
            "| time/               |          |\n",
            "|    episodes         | 704      |\n",
            "|    fps              | 656      |\n",
            "|    time_elapsed     | 103      |\n",
            "|    total_timesteps  | 68109    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.35     |\n",
            "|    n_updates        | 14527    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -101     |\n",
            "|    exploration_rate | 0.837    |\n",
            "| time/               |          |\n",
            "|    episodes         | 708      |\n",
            "|    fps              | 656      |\n",
            "|    time_elapsed     | 104      |\n",
            "|    total_timesteps  | 68601    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.442    |\n",
            "|    n_updates        | 14650    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -99.1    |\n",
            "|    exploration_rate | 0.836    |\n",
            "| time/               |          |\n",
            "|    episodes         | 712      |\n",
            "|    fps              | 656      |\n",
            "|    time_elapsed     | 105      |\n",
            "|    total_timesteps  | 68954    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 14738    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -96.8    |\n",
            "|    exploration_rate | 0.835    |\n",
            "| time/               |          |\n",
            "|    episodes         | 716      |\n",
            "|    fps              | 656      |\n",
            "|    time_elapsed     | 105      |\n",
            "|    total_timesteps  | 69272    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.385    |\n",
            "|    n_updates        | 14817    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -96.6    |\n",
            "|    exploration_rate | 0.834    |\n",
            "| time/               |          |\n",
            "|    episodes         | 720      |\n",
            "|    fps              | 656      |\n",
            "|    time_elapsed     | 106      |\n",
            "|    total_timesteps  | 69706    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 14926    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -93.8    |\n",
            "|    exploration_rate | 0.834    |\n",
            "| time/               |          |\n",
            "|    episodes         | 724      |\n",
            "|    fps              | 655      |\n",
            "|    time_elapsed     | 106      |\n",
            "|    total_timesteps  | 70070    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.567    |\n",
            "|    n_updates        | 15017    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -94.2    |\n",
            "|    exploration_rate | 0.833    |\n",
            "| time/               |          |\n",
            "|    episodes         | 728      |\n",
            "|    fps              | 655      |\n",
            "|    time_elapsed     | 107      |\n",
            "|    total_timesteps  | 70446    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.66     |\n",
            "|    n_updates        | 15111    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -94.9    |\n",
            "|    exploration_rate | 0.832    |\n",
            "| time/               |          |\n",
            "|    episodes         | 732      |\n",
            "|    fps              | 655      |\n",
            "|    time_elapsed     | 108      |\n",
            "|    total_timesteps  | 70919    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.87     |\n",
            "|    n_updates        | 15229    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -94.9    |\n",
            "|    exploration_rate | 0.831    |\n",
            "| time/               |          |\n",
            "|    episodes         | 736      |\n",
            "|    fps              | 654      |\n",
            "|    time_elapsed     | 108      |\n",
            "|    total_timesteps  | 71277    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.63     |\n",
            "|    n_updates        | 15319    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.1     |\n",
            "|    ep_rew_mean      | -95      |\n",
            "|    exploration_rate | 0.83     |\n",
            "| time/               |          |\n",
            "|    episodes         | 740      |\n",
            "|    fps              | 652      |\n",
            "|    time_elapsed     | 109      |\n",
            "|    total_timesteps  | 71633    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.25     |\n",
            "|    n_updates        | 15408    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.9     |\n",
            "|    ep_rew_mean      | -96.2    |\n",
            "|    exploration_rate | 0.829    |\n",
            "| time/               |          |\n",
            "|    episodes         | 744      |\n",
            "|    fps              | 651      |\n",
            "|    time_elapsed     | 110      |\n",
            "|    total_timesteps  | 71951    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.738    |\n",
            "|    n_updates        | 15487    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.5     |\n",
            "|    ep_rew_mean      | -97      |\n",
            "|    exploration_rate | 0.828    |\n",
            "| time/               |          |\n",
            "|    episodes         | 748      |\n",
            "|    fps              | 650      |\n",
            "|    time_elapsed     | 111      |\n",
            "|    total_timesteps  | 72367    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.228    |\n",
            "|    n_updates        | 15591    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.6     |\n",
            "|    ep_rew_mean      | -97.4    |\n",
            "|    exploration_rate | 0.827    |\n",
            "| time/               |          |\n",
            "|    episodes         | 752      |\n",
            "|    fps              | 650      |\n",
            "|    time_elapsed     | 111      |\n",
            "|    total_timesteps  | 72734    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.342    |\n",
            "|    n_updates        | 15683    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.5     |\n",
            "|    ep_rew_mean      | -96.9    |\n",
            "|    exploration_rate | 0.826    |\n",
            "| time/               |          |\n",
            "|    episodes         | 756      |\n",
            "|    fps              | 649      |\n",
            "|    time_elapsed     | 112      |\n",
            "|    total_timesteps  | 73119    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.35     |\n",
            "|    n_updates        | 15779    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.7     |\n",
            "|    ep_rew_mean      | -96.5    |\n",
            "|    exploration_rate | 0.826    |\n",
            "| time/               |          |\n",
            "|    episodes         | 760      |\n",
            "|    fps              | 649      |\n",
            "|    time_elapsed     | 112      |\n",
            "|    total_timesteps  | 73403    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.59     |\n",
            "|    n_updates        | 15850    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.1     |\n",
            "|    ep_rew_mean      | -95.9    |\n",
            "|    exploration_rate | 0.825    |\n",
            "| time/               |          |\n",
            "|    episodes         | 764      |\n",
            "|    fps              | 649      |\n",
            "|    time_elapsed     | 113      |\n",
            "|    total_timesteps  | 73790    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.739    |\n",
            "|    n_updates        | 15947    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.9     |\n",
            "|    ep_rew_mean      | -95.3    |\n",
            "|    exploration_rate | 0.824    |\n",
            "| time/               |          |\n",
            "|    episodes         | 768      |\n",
            "|    fps              | 649      |\n",
            "|    time_elapsed     | 114      |\n",
            "|    total_timesteps  | 74268    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 4.56     |\n",
            "|    n_updates        | 16066    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99       |\n",
            "|    ep_rew_mean      | -97.2    |\n",
            "|    exploration_rate | 0.823    |\n",
            "| time/               |          |\n",
            "|    episodes         | 772      |\n",
            "|    fps              | 648      |\n",
            "|    time_elapsed     | 115      |\n",
            "|    total_timesteps  | 74728    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.03     |\n",
            "|    n_updates        | 16181    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99       |\n",
            "|    ep_rew_mean      | -97      |\n",
            "|    exploration_rate | 0.822    |\n",
            "| time/               |          |\n",
            "|    episodes         | 776      |\n",
            "|    fps              | 648      |\n",
            "|    time_elapsed     | 115      |\n",
            "|    total_timesteps  | 75150    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.613    |\n",
            "|    n_updates        | 16287    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -98      |\n",
            "|    exploration_rate | 0.82     |\n",
            "| time/               |          |\n",
            "|    episodes         | 780      |\n",
            "|    fps              | 648      |\n",
            "|    time_elapsed     | 116      |\n",
            "|    total_timesteps  | 75662    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.325    |\n",
            "|    n_updates        | 16415    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.1     |\n",
            "|    ep_rew_mean      | -98      |\n",
            "|    exploration_rate | 0.819    |\n",
            "| time/               |          |\n",
            "|    episodes         | 784      |\n",
            "|    fps              | 648      |\n",
            "|    time_elapsed     | 117      |\n",
            "|    total_timesteps  | 76037    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.569    |\n",
            "|    n_updates        | 16509    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.8     |\n",
            "|    ep_rew_mean      | -95      |\n",
            "|    exploration_rate | 0.818    |\n",
            "| time/               |          |\n",
            "|    episodes         | 788      |\n",
            "|    fps              | 648      |\n",
            "|    time_elapsed     | 117      |\n",
            "|    total_timesteps  | 76431    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.467    |\n",
            "|    n_updates        | 16607    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.8     |\n",
            "|    ep_rew_mean      | -94      |\n",
            "|    exploration_rate | 0.818    |\n",
            "| time/               |          |\n",
            "|    episodes         | 792      |\n",
            "|    fps              | 647      |\n",
            "|    time_elapsed     | 118      |\n",
            "|    total_timesteps  | 76816    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.58     |\n",
            "|    n_updates        | 16703    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.6     |\n",
            "|    ep_rew_mean      | -95      |\n",
            "|    exploration_rate | 0.817    |\n",
            "| time/               |          |\n",
            "|    episodes         | 796      |\n",
            "|    fps              | 647      |\n",
            "|    time_elapsed     | 119      |\n",
            "|    total_timesteps  | 77177    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.639    |\n",
            "|    n_updates        | 16794    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.1     |\n",
            "|    ep_rew_mean      | -95.8    |\n",
            "|    exploration_rate | 0.816    |\n",
            "| time/               |          |\n",
            "|    episodes         | 800      |\n",
            "|    fps              | 647      |\n",
            "|    time_elapsed     | 119      |\n",
            "|    total_timesteps  | 77599    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.726    |\n",
            "|    n_updates        | 16899    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.9     |\n",
            "|    ep_rew_mean      | -96.4    |\n",
            "|    exploration_rate | 0.815    |\n",
            "| time/               |          |\n",
            "|    episodes         | 804      |\n",
            "|    fps              | 647      |\n",
            "|    time_elapsed     | 120      |\n",
            "|    total_timesteps  | 77996    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.31     |\n",
            "|    n_updates        | 16998    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.2     |\n",
            "|    ep_rew_mean      | -95.1    |\n",
            "|    exploration_rate | 0.814    |\n",
            "| time/               |          |\n",
            "|    episodes         | 808      |\n",
            "|    fps              | 647      |\n",
            "|    time_elapsed     | 121      |\n",
            "|    total_timesteps  | 78425    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.69     |\n",
            "|    n_updates        | 17106    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.4     |\n",
            "|    ep_rew_mean      | -94      |\n",
            "|    exploration_rate | 0.813    |\n",
            "| time/               |          |\n",
            "|    episodes         | 812      |\n",
            "|    fps              | 645      |\n",
            "|    time_elapsed     | 122      |\n",
            "|    total_timesteps  | 78798    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.26     |\n",
            "|    n_updates        | 17199    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -92.1    |\n",
            "|    exploration_rate | 0.81     |\n",
            "| time/               |          |\n",
            "|    episodes         | 816      |\n",
            "|    fps              | 636      |\n",
            "|    time_elapsed     | 125      |\n",
            "|    total_timesteps  | 80107    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 17526    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -93.1    |\n",
            "|    exploration_rate | 0.809    |\n",
            "| time/               |          |\n",
            "|    episodes         | 820      |\n",
            "|    fps              | 636      |\n",
            "|    time_elapsed     | 126      |\n",
            "|    total_timesteps  | 80558    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.54     |\n",
            "|    n_updates        | 17639    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -94      |\n",
            "|    exploration_rate | 0.808    |\n",
            "| time/               |          |\n",
            "|    episodes         | 824      |\n",
            "|    fps              | 636      |\n",
            "|    time_elapsed     | 127      |\n",
            "|    total_timesteps  | 80883    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.66     |\n",
            "|    n_updates        | 17720    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -90.3    |\n",
            "|    exploration_rate | 0.807    |\n",
            "| time/               |          |\n",
            "|    episodes         | 828      |\n",
            "|    fps              | 636      |\n",
            "|    time_elapsed     | 127      |\n",
            "|    total_timesteps  | 81161    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.35     |\n",
            "|    n_updates        | 17790    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -90      |\n",
            "|    exploration_rate | 0.806    |\n",
            "| time/               |          |\n",
            "|    episodes         | 832      |\n",
            "|    fps              | 636      |\n",
            "|    time_elapsed     | 128      |\n",
            "|    total_timesteps  | 81589    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.879    |\n",
            "|    n_updates        | 17897    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -91.7    |\n",
            "|    exploration_rate | 0.805    |\n",
            "| time/               |          |\n",
            "|    episodes         | 836      |\n",
            "|    fps              | 636      |\n",
            "|    time_elapsed     | 128      |\n",
            "|    total_timesteps  | 82031    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.495    |\n",
            "|    n_updates        | 18007    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -91.8    |\n",
            "|    exploration_rate | 0.804    |\n",
            "| time/               |          |\n",
            "|    episodes         | 840      |\n",
            "|    fps              | 636      |\n",
            "|    time_elapsed     | 129      |\n",
            "|    total_timesteps  | 82404    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.58     |\n",
            "|    n_updates        | 18100    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -91.5    |\n",
            "|    exploration_rate | 0.803    |\n",
            "| time/               |          |\n",
            "|    episodes         | 844      |\n",
            "|    fps              | 635      |\n",
            "|    time_elapsed     | 130      |\n",
            "|    total_timesteps  | 82811    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.42     |\n",
            "|    n_updates        | 18202    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -92.7    |\n",
            "|    exploration_rate | 0.802    |\n",
            "| time/               |          |\n",
            "|    episodes         | 848      |\n",
            "|    fps              | 635      |\n",
            "|    time_elapsed     | 130      |\n",
            "|    total_timesteps  | 83218    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.396    |\n",
            "|    n_updates        | 18304    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -91.7    |\n",
            "|    exploration_rate | 0.801    |\n",
            "| time/               |          |\n",
            "|    episodes         | 852      |\n",
            "|    fps              | 635      |\n",
            "|    time_elapsed     | 131      |\n",
            "|    total_timesteps  | 83630    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.382    |\n",
            "|    n_updates        | 18407    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -92.1    |\n",
            "|    exploration_rate | 0.8      |\n",
            "| time/               |          |\n",
            "|    episodes         | 856      |\n",
            "|    fps              | 635      |\n",
            "|    time_elapsed     | 132      |\n",
            "|    total_timesteps  | 84009    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.12     |\n",
            "|    n_updates        | 18502    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -92.4    |\n",
            "|    exploration_rate | 0.8      |\n",
            "| time/               |          |\n",
            "|    episodes         | 860      |\n",
            "|    fps              | 635      |\n",
            "|    time_elapsed     | 132      |\n",
            "|    total_timesteps  | 84309    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.6      |\n",
            "|    n_updates        | 18577    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -91.5    |\n",
            "|    exploration_rate | 0.799    |\n",
            "| time/               |          |\n",
            "|    episodes         | 864      |\n",
            "|    fps              | 635      |\n",
            "|    time_elapsed     | 133      |\n",
            "|    total_timesteps  | 84793    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.646    |\n",
            "|    n_updates        | 18698    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -90.7    |\n",
            "|    exploration_rate | 0.798    |\n",
            "| time/               |          |\n",
            "|    episodes         | 868      |\n",
            "|    fps              | 633      |\n",
            "|    time_elapsed     | 134      |\n",
            "|    total_timesteps  | 85205    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.26     |\n",
            "|    n_updates        | 18801    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -88.4    |\n",
            "|    exploration_rate | 0.797    |\n",
            "| time/               |          |\n",
            "|    episodes         | 872      |\n",
            "|    fps              | 632      |\n",
            "|    time_elapsed     | 135      |\n",
            "|    total_timesteps  | 85564    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.781    |\n",
            "|    n_updates        | 18890    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -89.1    |\n",
            "|    exploration_rate | 0.796    |\n",
            "| time/               |          |\n",
            "|    episodes         | 876      |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 135      |\n",
            "|    total_timesteps  | 85903    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.235    |\n",
            "|    n_updates        | 18975    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -87.4    |\n",
            "|    exploration_rate | 0.795    |\n",
            "| time/               |          |\n",
            "|    episodes         | 880      |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 136      |\n",
            "|    total_timesteps  | 86375    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.24     |\n",
            "|    n_updates        | 19093    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -87.8    |\n",
            "|    exploration_rate | 0.794    |\n",
            "| time/               |          |\n",
            "|    episodes         | 884      |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 137      |\n",
            "|    total_timesteps  | 86747    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 19186    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -90.5    |\n",
            "|    exploration_rate | 0.793    |\n",
            "| time/               |          |\n",
            "|    episodes         | 888      |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 138      |\n",
            "|    total_timesteps  | 87160    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.56     |\n",
            "|    n_updates        | 19289    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -89.8    |\n",
            "|    exploration_rate | 0.792    |\n",
            "| time/               |          |\n",
            "|    episodes         | 892      |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 138      |\n",
            "|    total_timesteps  | 87554    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.398    |\n",
            "|    n_updates        | 19388    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -88.3    |\n",
            "|    exploration_rate | 0.791    |\n",
            "| time/               |          |\n",
            "|    episodes         | 896      |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 139      |\n",
            "|    total_timesteps  | 87923    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.19     |\n",
            "|    n_updates        | 19480    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -89.4    |\n",
            "|    exploration_rate | 0.79     |\n",
            "| time/               |          |\n",
            "|    episodes         | 900      |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 140      |\n",
            "|    total_timesteps  | 88361    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.31     |\n",
            "|    n_updates        | 19590    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -88.4    |\n",
            "|    exploration_rate | 0.789    |\n",
            "| time/               |          |\n",
            "|    episodes         | 904      |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 140      |\n",
            "|    total_timesteps  | 88707    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 19676    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -89.2    |\n",
            "|    exploration_rate | 0.788    |\n",
            "| time/               |          |\n",
            "|    episodes         | 908      |\n",
            "|    fps              | 631      |\n",
            "|    time_elapsed     | 141      |\n",
            "|    total_timesteps  | 89069    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.19     |\n",
            "|    n_updates        | 19767    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -90.2    |\n",
            "|    exploration_rate | 0.788    |\n",
            "| time/               |          |\n",
            "|    episodes         | 912      |\n",
            "|    fps              | 630      |\n",
            "|    time_elapsed     | 141      |\n",
            "|    total_timesteps  | 89465    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.4      |\n",
            "|    n_updates        | 19866    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.5     |\n",
            "|    ep_rew_mean      | -92.2    |\n",
            "|    exploration_rate | 0.786    |\n",
            "| time/               |          |\n",
            "|    episodes         | 916      |\n",
            "|    fps              | 630      |\n",
            "|    time_elapsed     | 142      |\n",
            "|    total_timesteps  | 89955    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.862    |\n",
            "|    n_updates        | 19988    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.2     |\n",
            "|    ep_rew_mean      | -91.7    |\n",
            "|    exploration_rate | 0.785    |\n",
            "| time/               |          |\n",
            "|    episodes         | 920      |\n",
            "|    fps              | 630      |\n",
            "|    time_elapsed     | 143      |\n",
            "|    total_timesteps  | 90474    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.629    |\n",
            "|    n_updates        | 20118    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -92.2    |\n",
            "|    exploration_rate | 0.784    |\n",
            "| time/               |          |\n",
            "|    episodes         | 924      |\n",
            "|    fps              | 630      |\n",
            "|    time_elapsed     | 144      |\n",
            "|    total_timesteps  | 90886    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.462    |\n",
            "|    n_updates        | 20221    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -95      |\n",
            "|    exploration_rate | 0.783    |\n",
            "| time/               |          |\n",
            "|    episodes         | 928      |\n",
            "|    fps              | 630      |\n",
            "|    time_elapsed     | 144      |\n",
            "|    total_timesteps  | 91256    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.394    |\n",
            "|    n_updates        | 20313    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -95.6    |\n",
            "|    exploration_rate | 0.782    |\n",
            "| time/               |          |\n",
            "|    episodes         | 932      |\n",
            "|    fps              | 630      |\n",
            "|    time_elapsed     | 145      |\n",
            "|    total_timesteps  | 91663    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.575    |\n",
            "|    n_updates        | 20415    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.7     |\n",
            "|    ep_rew_mean      | -92      |\n",
            "|    exploration_rate | 0.781    |\n",
            "| time/               |          |\n",
            "|    episodes         | 936      |\n",
            "|    fps              | 630      |\n",
            "|    time_elapsed     | 145      |\n",
            "|    total_timesteps  | 92001    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.95     |\n",
            "|    n_updates        | 20500    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -91.6    |\n",
            "|    exploration_rate | 0.78     |\n",
            "| time/               |          |\n",
            "|    episodes         | 940      |\n",
            "|    fps              | 629      |\n",
            "|    time_elapsed     | 147      |\n",
            "|    total_timesteps  | 92494    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.353    |\n",
            "|    n_updates        | 20623    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -90.6    |\n",
            "|    exploration_rate | 0.779    |\n",
            "| time/               |          |\n",
            "|    episodes         | 944      |\n",
            "|    fps              | 627      |\n",
            "|    time_elapsed     | 147      |\n",
            "|    total_timesteps  | 92892    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.22     |\n",
            "|    n_updates        | 20722    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -88.7    |\n",
            "|    exploration_rate | 0.779    |\n",
            "| time/               |          |\n",
            "|    episodes         | 948      |\n",
            "|    fps              | 626      |\n",
            "|    time_elapsed     | 148      |\n",
            "|    total_timesteps  | 93246    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.6      |\n",
            "|    n_updates        | 20811    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -88.4    |\n",
            "|    exploration_rate | 0.778    |\n",
            "| time/               |          |\n",
            "|    episodes         | 952      |\n",
            "|    fps              | 626      |\n",
            "|    time_elapsed     | 149      |\n",
            "|    total_timesteps  | 93672    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.4      |\n",
            "|    n_updates        | 20917    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -89.9    |\n",
            "|    exploration_rate | 0.777    |\n",
            "| time/               |          |\n",
            "|    episodes         | 956      |\n",
            "|    fps              | 626      |\n",
            "|    time_elapsed     | 150      |\n",
            "|    total_timesteps  | 94034    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.11     |\n",
            "|    n_updates        | 21008    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -90.8    |\n",
            "|    exploration_rate | 0.776    |\n",
            "| time/               |          |\n",
            "|    episodes         | 960      |\n",
            "|    fps              | 626      |\n",
            "|    time_elapsed     | 150      |\n",
            "|    total_timesteps  | 94473    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.36     |\n",
            "|    n_updates        | 21118    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -91      |\n",
            "|    exploration_rate | 0.775    |\n",
            "| time/               |          |\n",
            "|    episodes         | 964      |\n",
            "|    fps              | 625      |\n",
            "|    time_elapsed     | 151      |\n",
            "|    total_timesteps  | 94858    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.8      |\n",
            "|    n_updates        | 21214    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -90.3    |\n",
            "|    exploration_rate | 0.774    |\n",
            "| time/               |          |\n",
            "|    episodes         | 968      |\n",
            "|    fps              | 625      |\n",
            "|    time_elapsed     | 152      |\n",
            "|    total_timesteps  | 95248    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.618    |\n",
            "|    n_updates        | 21311    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -90.5    |\n",
            "|    exploration_rate | 0.773    |\n",
            "| time/               |          |\n",
            "|    episodes         | 972      |\n",
            "|    fps              | 625      |\n",
            "|    time_elapsed     | 152      |\n",
            "|    total_timesteps  | 95604    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.37     |\n",
            "|    n_updates        | 21400    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -88.8    |\n",
            "|    exploration_rate | 0.77     |\n",
            "| time/               |          |\n",
            "|    episodes         | 976      |\n",
            "|    fps              | 619      |\n",
            "|    time_elapsed     | 156      |\n",
            "|    total_timesteps  | 96897    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.48     |\n",
            "|    n_updates        | 21724    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -89.3    |\n",
            "|    exploration_rate | 0.769    |\n",
            "| time/               |          |\n",
            "|    episodes         | 980      |\n",
            "|    fps              | 619      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 97287    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.908    |\n",
            "|    n_updates        | 21821    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -89.3    |\n",
            "|    exploration_rate | 0.768    |\n",
            "| time/               |          |\n",
            "|    episodes         | 984      |\n",
            "|    fps              | 619      |\n",
            "|    time_elapsed     | 157      |\n",
            "|    total_timesteps  | 97680    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.44     |\n",
            "|    n_updates        | 21919    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -86.3    |\n",
            "|    exploration_rate | 0.767    |\n",
            "| time/               |          |\n",
            "|    episodes         | 988      |\n",
            "|    fps              | 619      |\n",
            "|    time_elapsed     | 158      |\n",
            "|    total_timesteps  | 98095    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.686    |\n",
            "|    n_updates        | 22023    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -85.2    |\n",
            "|    exploration_rate | 0.766    |\n",
            "| time/               |          |\n",
            "|    episodes         | 992      |\n",
            "|    fps              | 618      |\n",
            "|    time_elapsed     | 159      |\n",
            "|    total_timesteps  | 98575    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.59     |\n",
            "|    n_updates        | 22143    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -84.6    |\n",
            "|    exploration_rate | 0.765    |\n",
            "| time/               |          |\n",
            "|    episodes         | 996      |\n",
            "|    fps              | 617      |\n",
            "|    time_elapsed     | 160      |\n",
            "|    total_timesteps  | 99003    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.443    |\n",
            "|    n_updates        | 22250    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -82.7    |\n",
            "|    exploration_rate | 0.764    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1000     |\n",
            "|    fps              | 616      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 99409    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.76     |\n",
            "|    n_updates        | 22352    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -82.9    |\n",
            "|    exploration_rate | 0.763    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1004     |\n",
            "|    fps              | 615      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 99780    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.455    |\n",
            "|    n_updates        | 22444    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -82.2    |\n",
            "|    exploration_rate | 0.762    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1008     |\n",
            "|    fps              | 615      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 100119   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.81     |\n",
            "|    n_updates        | 22529    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -81.4    |\n",
            "|    exploration_rate | 0.761    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1012     |\n",
            "|    fps              | 615      |\n",
            "|    time_elapsed     | 163      |\n",
            "|    total_timesteps  | 100564   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.406    |\n",
            "|    n_updates        | 22640    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -81.9    |\n",
            "|    exploration_rate | 0.76     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1016     |\n",
            "|    fps              | 615      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 100882   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.34     |\n",
            "|    n_updates        | 22720    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -80.8    |\n",
            "|    exploration_rate | 0.76     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1020     |\n",
            "|    fps              | 615      |\n",
            "|    time_elapsed     | 164      |\n",
            "|    total_timesteps  | 101243   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.369    |\n",
            "|    n_updates        | 22810    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -80.2    |\n",
            "|    exploration_rate | 0.759    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1024     |\n",
            "|    fps              | 615      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 101626   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.589    |\n",
            "|    n_updates        | 22906    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -79.7    |\n",
            "|    exploration_rate | 0.758    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1028     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 165      |\n",
            "|    total_timesteps  | 102047   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.518    |\n",
            "|    n_updates        | 23011    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -79.6    |\n",
            "|    exploration_rate | 0.757    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1032     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 166      |\n",
            "|    total_timesteps  | 102407   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.48     |\n",
            "|    n_updates        | 23101    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -79.8    |\n",
            "|    exploration_rate | 0.756    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1036     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 102845   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.81     |\n",
            "|    n_updates        | 23211    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -78.8    |\n",
            "|    exploration_rate | 0.755    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1040     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 167      |\n",
            "|    total_timesteps  | 103236   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.644    |\n",
            "|    n_updates        | 23308    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -77.6    |\n",
            "|    exploration_rate | 0.754    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1044     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 168      |\n",
            "|    total_timesteps  | 103599   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.2      |\n",
            "|    n_updates        | 23399    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -75.7    |\n",
            "|    exploration_rate | 0.753    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1048     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 169      |\n",
            "|    total_timesteps  | 104006   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.28     |\n",
            "|    n_updates        | 23501    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -76.3    |\n",
            "|    exploration_rate | 0.752    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1052     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 169      |\n",
            "|    total_timesteps  | 104445   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.829    |\n",
            "|    n_updates        | 23611    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -72.3    |\n",
            "|    exploration_rate | 0.751    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1056     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 170      |\n",
            "|    total_timesteps  | 104944   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.256    |\n",
            "|    n_updates        | 23735    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -71      |\n",
            "|    exploration_rate | 0.75     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1060     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 171      |\n",
            "|    total_timesteps  | 105318   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.04     |\n",
            "|    n_updates        | 23829    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -72.4    |\n",
            "|    exploration_rate | 0.749    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1064     |\n",
            "|    fps              | 614      |\n",
            "|    time_elapsed     | 172      |\n",
            "|    total_timesteps  | 105745   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.47     |\n",
            "|    n_updates        | 23936    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -71.7    |\n",
            "|    exploration_rate | 0.748    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1068     |\n",
            "|    fps              | 613      |\n",
            "|    time_elapsed     | 173      |\n",
            "|    total_timesteps  | 106190   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.268    |\n",
            "|    n_updates        | 24047    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -72.7    |\n",
            "|    exploration_rate | 0.747    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1072     |\n",
            "|    fps              | 613      |\n",
            "|    time_elapsed     | 173      |\n",
            "|    total_timesteps  | 106480   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.64     |\n",
            "|    n_updates        | 24119    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.7     |\n",
            "|    ep_rew_mean      | -74.2    |\n",
            "|    exploration_rate | 0.746    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1076     |\n",
            "|    fps              | 612      |\n",
            "|    time_elapsed     | 174      |\n",
            "|    total_timesteps  | 106865   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.24     |\n",
            "|    n_updates        | 24216    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.4     |\n",
            "|    ep_rew_mean      | -73.8    |\n",
            "|    exploration_rate | 0.745    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1080     |\n",
            "|    fps              | 612      |\n",
            "|    time_elapsed     | 175      |\n",
            "|    total_timesteps  | 107231   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.05     |\n",
            "|    n_updates        | 24307    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.5     |\n",
            "|    ep_rew_mean      | -73.1    |\n",
            "|    exploration_rate | 0.744    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1084     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 175      |\n",
            "|    total_timesteps  | 107628   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.245    |\n",
            "|    n_updates        | 24406    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.9     |\n",
            "|    ep_rew_mean      | -72.9    |\n",
            "|    exploration_rate | 0.744    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1088     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 176      |\n",
            "|    total_timesteps  | 107987   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.496    |\n",
            "|    n_updates        | 24496    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.9     |\n",
            "|    ep_rew_mean      | -74.1    |\n",
            "|    exploration_rate | 0.743    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1092     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 177      |\n",
            "|    total_timesteps  | 108364   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.464    |\n",
            "|    n_updates        | 24590    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.5     |\n",
            "|    ep_rew_mean      | -75.5    |\n",
            "|    exploration_rate | 0.742    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1096     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 177      |\n",
            "|    total_timesteps  | 108749   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.42     |\n",
            "|    n_updates        | 24687    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.9     |\n",
            "|    ep_rew_mean      | -75.5    |\n",
            "|    exploration_rate | 0.741    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1100     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 178      |\n",
            "|    total_timesteps  | 109195   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.826    |\n",
            "|    n_updates        | 24798    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 97.8     |\n",
            "|    ep_rew_mean      | -75.4    |\n",
            "|    exploration_rate | 0.74     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1104     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 179      |\n",
            "|    total_timesteps  | 109558   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.986    |\n",
            "|    n_updates        | 24889    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.6     |\n",
            "|    ep_rew_mean      | -75.3    |\n",
            "|    exploration_rate | 0.739    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1108     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 179      |\n",
            "|    total_timesteps  | 109983   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.02     |\n",
            "|    n_updates        | 24995    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 98.8     |\n",
            "|    ep_rew_mean      | -75.8    |\n",
            "|    exploration_rate | 0.738    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1112     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 180      |\n",
            "|    total_timesteps  | 110439   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.312    |\n",
            "|    n_updates        | 25109    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -74.8    |\n",
            "|    exploration_rate | 0.737    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1116     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 181      |\n",
            "|    total_timesteps  | 110925   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 4.68     |\n",
            "|    n_updates        | 25231    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -75.8    |\n",
            "|    exploration_rate | 0.736    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1120     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 182      |\n",
            "|    total_timesteps  | 111345   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.304    |\n",
            "|    n_updates        | 25336    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -76.3    |\n",
            "|    exploration_rate | 0.735    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1124     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 182      |\n",
            "|    total_timesteps  | 111692   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.53     |\n",
            "|    n_updates        | 25422    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -75.5    |\n",
            "|    exploration_rate | 0.733    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1128     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 183      |\n",
            "|    total_timesteps  | 112238   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.406    |\n",
            "|    n_updates        | 25559    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -74      |\n",
            "|    exploration_rate | 0.732    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1132     |\n",
            "|    fps              | 611      |\n",
            "|    time_elapsed     | 184      |\n",
            "|    total_timesteps  | 112648   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.841    |\n",
            "|    n_updates        | 25661    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -74      |\n",
            "|    exploration_rate | 0.732    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1136     |\n",
            "|    fps              | 610      |\n",
            "|    time_elapsed     | 185      |\n",
            "|    total_timesteps  | 112986   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 4.62     |\n",
            "|    n_updates        | 25746    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -74.6    |\n",
            "|    exploration_rate | 0.731    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1140     |\n",
            "|    fps              | 609      |\n",
            "|    time_elapsed     | 186      |\n",
            "|    total_timesteps  | 113431   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.07     |\n",
            "|    n_updates        | 25857    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -75.4    |\n",
            "|    exploration_rate | 0.73     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1144     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 187      |\n",
            "|    total_timesteps  | 113830   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.84     |\n",
            "|    n_updates        | 25957    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -76.1    |\n",
            "|    exploration_rate | 0.729    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1148     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 187      |\n",
            "|    total_timesteps  | 114254   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.624    |\n",
            "|    n_updates        | 26063    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -75.8    |\n",
            "|    exploration_rate | 0.728    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1152     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 188      |\n",
            "|    total_timesteps  | 114632   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.33     |\n",
            "|    n_updates        | 26157    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -76.4    |\n",
            "|    exploration_rate | 0.727    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1156     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 189      |\n",
            "|    total_timesteps  | 115002   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.92     |\n",
            "|    n_updates        | 26250    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -75.9    |\n",
            "|    exploration_rate | 0.726    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1160     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 189      |\n",
            "|    total_timesteps  | 115337   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.5      |\n",
            "|    n_updates        | 26334    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -75.3    |\n",
            "|    exploration_rate | 0.725    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1164     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 190      |\n",
            "|    total_timesteps  | 115759   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 26439    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.5     |\n",
            "|    ep_rew_mean      | -75      |\n",
            "|    exploration_rate | 0.724    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1168     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 190      |\n",
            "|    total_timesteps  | 116141   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.428    |\n",
            "|    n_updates        | 26535    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.9     |\n",
            "|    ep_rew_mean      | -73.6    |\n",
            "|    exploration_rate | 0.723    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1172     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 191      |\n",
            "|    total_timesteps  | 116467   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.85     |\n",
            "|    n_updates        | 26616    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -74      |\n",
            "|    exploration_rate | 0.722    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1176     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 192      |\n",
            "|    total_timesteps  | 116925   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.469    |\n",
            "|    n_updates        | 26731    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -74.3    |\n",
            "|    exploration_rate | 0.721    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1180     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 192      |\n",
            "|    total_timesteps  | 117303   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.47     |\n",
            "|    n_updates        | 26825    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -74.9    |\n",
            "|    exploration_rate | 0.72     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1184     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 193      |\n",
            "|    total_timesteps  | 117723   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.05     |\n",
            "|    n_updates        | 26930    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -74.6    |\n",
            "|    exploration_rate | 0.719    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1188     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 194      |\n",
            "|    total_timesteps  | 118141   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 5.48     |\n",
            "|    n_updates        | 27035    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -73.3    |\n",
            "|    exploration_rate | 0.718    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1192     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 194      |\n",
            "|    total_timesteps  | 118549   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.412    |\n",
            "|    n_updates        | 27137    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -72.1    |\n",
            "|    exploration_rate | 0.718    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1196     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 195      |\n",
            "|    total_timesteps  | 118894   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.54     |\n",
            "|    n_updates        | 27223    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -72.1    |\n",
            "|    exploration_rate | 0.717    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1200     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 196      |\n",
            "|    total_timesteps  | 119333   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.473    |\n",
            "|    n_updates        | 27333    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -72      |\n",
            "|    exploration_rate | 0.715    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1204     |\n",
            "|    fps              | 608      |\n",
            "|    time_elapsed     | 197      |\n",
            "|    total_timesteps  | 119856   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.443    |\n",
            "|    n_updates        | 27463    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -71.2    |\n",
            "|    exploration_rate | 0.714    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1208     |\n",
            "|    fps              | 607      |\n",
            "|    time_elapsed     | 197      |\n",
            "|    total_timesteps  | 120234   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.805    |\n",
            "|    n_updates        | 27558    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -69.2    |\n",
            "|    exploration_rate | 0.714    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1212     |\n",
            "|    fps              | 606      |\n",
            "|    time_elapsed     | 198      |\n",
            "|    total_timesteps  | 120583   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.82     |\n",
            "|    n_updates        | 27645    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -69.7    |\n",
            "|    exploration_rate | 0.713    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1216     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 199      |\n",
            "|    total_timesteps  | 120977   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.617    |\n",
            "|    n_updates        | 27744    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -70.1    |\n",
            "|    exploration_rate | 0.712    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1220     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 200      |\n",
            "|    total_timesteps  | 121390   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.551    |\n",
            "|    n_updates        | 27847    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -68.9    |\n",
            "|    exploration_rate | 0.711    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1224     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 201      |\n",
            "|    total_timesteps  | 121808   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.347    |\n",
            "|    n_updates        | 27951    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.6     |\n",
            "|    ep_rew_mean      | -69.3    |\n",
            "|    exploration_rate | 0.71     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1228     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 201      |\n",
            "|    total_timesteps  | 122202   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.218    |\n",
            "|    n_updates        | 28050    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 99.7     |\n",
            "|    ep_rew_mean      | -69.1    |\n",
            "|    exploration_rate | 0.709    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1232     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 202      |\n",
            "|    total_timesteps  | 122613   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.547    |\n",
            "|    n_updates        | 28153    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -69.5    |\n",
            "|    exploration_rate | 0.708    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1236     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 203      |\n",
            "|    total_timesteps  | 123065   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.64     |\n",
            "|    n_updates        | 28266    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 100      |\n",
            "|    ep_rew_mean      | -70      |\n",
            "|    exploration_rate | 0.707    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1240     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 203      |\n",
            "|    total_timesteps  | 123456   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.3      |\n",
            "|    n_updates        | 28363    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -68.8    |\n",
            "|    exploration_rate | 0.706    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1244     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 204      |\n",
            "|    total_timesteps  | 123884   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.526    |\n",
            "|    n_updates        | 28470    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -68      |\n",
            "|    exploration_rate | 0.705    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1248     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 205      |\n",
            "|    total_timesteps  | 124374   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.73     |\n",
            "|    n_updates        | 28593    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -67.5    |\n",
            "|    exploration_rate | 0.704    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1252     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 206      |\n",
            "|    total_timesteps  | 124725   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.631    |\n",
            "|    n_updates        | 28681    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -67.9    |\n",
            "|    exploration_rate | 0.703    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1256     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 206      |\n",
            "|    total_timesteps  | 125228   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.07     |\n",
            "|    n_updates        | 28806    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -68      |\n",
            "|    exploration_rate | 0.702    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1260     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 207      |\n",
            "|    total_timesteps  | 125610   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.313    |\n",
            "|    n_updates        | 28902    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -67      |\n",
            "|    exploration_rate | 0.701    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1264     |\n",
            "|    fps              | 604      |\n",
            "|    time_elapsed     | 208      |\n",
            "|    total_timesteps  | 126030   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 4.48     |\n",
            "|    n_updates        | 29007    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -66.1    |\n",
            "|    exploration_rate | 0.7      |\n",
            "| time/               |          |\n",
            "|    episodes         | 1268     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 208      |\n",
            "|    total_timesteps  | 126420   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.29     |\n",
            "|    n_updates        | 29104    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -66.8    |\n",
            "|    exploration_rate | 0.699    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1272     |\n",
            "|    fps              | 605      |\n",
            "|    time_elapsed     | 209      |\n",
            "|    total_timesteps  | 126891   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.931    |\n",
            "|    n_updates        | 29222    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -64.8    |\n",
            "|    exploration_rate | 0.697    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1276     |\n",
            "|    fps              | 604      |\n",
            "|    time_elapsed     | 210      |\n",
            "|    total_timesteps  | 127431   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 2.47     |\n",
            "|    n_updates        | 29357    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -64.6    |\n",
            "|    exploration_rate | 0.696    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1280     |\n",
            "|    fps              | 603      |\n",
            "|    time_elapsed     | 211      |\n",
            "|    total_timesteps  | 127847   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.698    |\n",
            "|    n_updates        | 29461    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -64      |\n",
            "|    exploration_rate | 0.695    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1284     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 212      |\n",
            "|    total_timesteps  | 128243   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.373    |\n",
            "|    n_updates        | 29560    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -63.1    |\n",
            "|    exploration_rate | 0.695    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1288     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 213      |\n",
            "|    total_timesteps  | 128591   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.87     |\n",
            "|    n_updates        | 29647    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -62.1    |\n",
            "|    exploration_rate | 0.694    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1292     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 214      |\n",
            "|    total_timesteps  | 129050   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.341    |\n",
            "|    n_updates        | 29762    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -62.7    |\n",
            "|    exploration_rate | 0.693    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1296     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 214      |\n",
            "|    total_timesteps  | 129467   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.49     |\n",
            "|    n_updates        | 29866    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -61      |\n",
            "|    exploration_rate | 0.692    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1300     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 215      |\n",
            "|    total_timesteps  | 129875   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.692    |\n",
            "|    n_updates        | 29968    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -60.1    |\n",
            "|    exploration_rate | 0.691    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1304     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 216      |\n",
            "|    total_timesteps  | 130224   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.457    |\n",
            "|    n_updates        | 30055    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -60.5    |\n",
            "|    exploration_rate | 0.69     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1308     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 216      |\n",
            "|    total_timesteps  | 130563   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.209    |\n",
            "|    n_updates        | 30140    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -62.5    |\n",
            "|    exploration_rate | 0.689    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1312     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 217      |\n",
            "|    total_timesteps  | 130889   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.755    |\n",
            "|    n_updates        | 30222    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -60.9    |\n",
            "|    exploration_rate | 0.688    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1316     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 217      |\n",
            "|    total_timesteps  | 131231   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.949    |\n",
            "|    n_updates        | 30307    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -58.8    |\n",
            "|    exploration_rate | 0.687    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1320     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 218      |\n",
            "|    total_timesteps  | 131673   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.423    |\n",
            "|    n_updates        | 30418    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -58.3    |\n",
            "|    exploration_rate | 0.686    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1324     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 219      |\n",
            "|    total_timesteps  | 132098   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.907    |\n",
            "|    n_updates        | 30524    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -57.3    |\n",
            "|    exploration_rate | 0.685    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1328     |\n",
            "|    fps              | 602      |\n",
            "|    time_elapsed     | 220      |\n",
            "|    total_timesteps  | 132584   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 4.34     |\n",
            "|    n_updates        | 30645    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -57.8    |\n",
            "|    exploration_rate | 0.684    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1332     |\n",
            "|    fps              | 601      |\n",
            "|    time_elapsed     | 221      |\n",
            "|    total_timesteps  | 133010   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.798    |\n",
            "|    n_updates        | 30752    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -56.6    |\n",
            "|    exploration_rate | 0.683    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1336     |\n",
            "|    fps              | 601      |\n",
            "|    time_elapsed     | 221      |\n",
            "|    total_timesteps  | 133441   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.711    |\n",
            "|    n_updates        | 30860    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -55.5    |\n",
            "|    exploration_rate | 0.682    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1340     |\n",
            "|    fps              | 601      |\n",
            "|    time_elapsed     | 222      |\n",
            "|    total_timesteps  | 133843   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 3.35     |\n",
            "|    n_updates        | 30960    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -56.3    |\n",
            "|    exploration_rate | 0.681    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1344     |\n",
            "|    fps              | 600      |\n",
            "|    time_elapsed     | 223      |\n",
            "|    total_timesteps  | 134254   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.471    |\n",
            "|    n_updates        | 31063    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -55.6    |\n",
            "|    exploration_rate | 0.68     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1348     |\n",
            "|    fps              | 599      |\n",
            "|    time_elapsed     | 224      |\n",
            "|    total_timesteps  | 134691   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.71     |\n",
            "|    n_updates        | 31172    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 104      |\n",
            "|    ep_rew_mean      | -56.5    |\n",
            "|    exploration_rate | 0.679    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1352     |\n",
            "|    fps              | 599      |\n",
            "|    time_elapsed     | 225      |\n",
            "|    total_timesteps  | 135100   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.992    |\n",
            "|    n_updates        | 31274    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -56.2    |\n",
            "|    exploration_rate | 0.678    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1356     |\n",
            "|    fps              | 599      |\n",
            "|    time_elapsed     | 226      |\n",
            "|    total_timesteps  | 135482   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.754    |\n",
            "|    n_updates        | 31370    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -56.2    |\n",
            "|    exploration_rate | 0.677    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1360     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 226      |\n",
            "|    total_timesteps  | 135851   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.54     |\n",
            "|    n_updates        | 31462    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -56.9    |\n",
            "|    exploration_rate | 0.676    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1364     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 227      |\n",
            "|    total_timesteps  | 136309   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.445    |\n",
            "|    n_updates        | 31577    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 103      |\n",
            "|    ep_rew_mean      | -56.4    |\n",
            "|    exploration_rate | 0.675    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1368     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 228      |\n",
            "|    total_timesteps  | 136677   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.372    |\n",
            "|    n_updates        | 31669    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -56.8    |\n",
            "|    exploration_rate | 0.674    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1372     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 228      |\n",
            "|    total_timesteps  | 137088   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.509    |\n",
            "|    n_updates        | 31771    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -57.3    |\n",
            "|    exploration_rate | 0.673    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1376     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 229      |\n",
            "|    total_timesteps  | 137509   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.248    |\n",
            "|    n_updates        | 31877    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -56.8    |\n",
            "|    exploration_rate | 0.672    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1380     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 230      |\n",
            "|    total_timesteps  | 137912   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.383    |\n",
            "|    n_updates        | 31977    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -57      |\n",
            "|    exploration_rate | 0.672    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1384     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 231      |\n",
            "|    total_timesteps  | 138310   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.55     |\n",
            "|    n_updates        | 32077    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -58      |\n",
            "|    exploration_rate | 0.671    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1388     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 231      |\n",
            "|    total_timesteps  | 138724   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.977    |\n",
            "|    n_updates        | 32180    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -57.6    |\n",
            "|    exploration_rate | 0.67     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1392     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 232      |\n",
            "|    total_timesteps  | 139152   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.87     |\n",
            "|    n_updates        | 32287    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 101      |\n",
            "|    ep_rew_mean      | -56.5    |\n",
            "|    exploration_rate | 0.668    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1396     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 233      |\n",
            "|    total_timesteps  | 139614   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.786    |\n",
            "|    n_updates        | 32403    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -58      |\n",
            "|    exploration_rate | 0.667    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1400     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 234      |\n",
            "|    total_timesteps  | 140034   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.572    |\n",
            "|    n_updates        | 32508    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 102      |\n",
            "|    ep_rew_mean      | -57.2    |\n",
            "|    exploration_rate | 0.666    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1404     |\n",
            "|    fps              | 598      |\n",
            "|    time_elapsed     | 234      |\n",
            "|    total_timesteps  | 140446   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.21     |\n",
            "|    n_updates        | 32611    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -58.3    |\n",
            "|    exploration_rate | 0.664    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1408     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 237      |\n",
            "|    total_timesteps  | 141320   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.709    |\n",
            "|    n_updates        | 32829    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -56.7    |\n",
            "|    exploration_rate | 0.663    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1412     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 238      |\n",
            "|    total_timesteps  | 141795   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.628    |\n",
            "|    n_updates        | 32948    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -56.4    |\n",
            "|    exploration_rate | 0.662    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1416     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 238      |\n",
            "|    total_timesteps  | 142149   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 7.6      |\n",
            "|    n_updates        | 33037    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -56.8    |\n",
            "|    exploration_rate | 0.661    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1420     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 239      |\n",
            "|    total_timesteps  | 142539   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.808    |\n",
            "|    n_updates        | 33134    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -57.3    |\n",
            "|    exploration_rate | 0.661    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1424     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 240      |\n",
            "|    total_timesteps  | 142901   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.6      |\n",
            "|    n_updates        | 33225    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 108      |\n",
            "|    ep_rew_mean      | -56.8    |\n",
            "|    exploration_rate | 0.659    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1428     |\n",
            "|    fps              | 595      |\n",
            "|    time_elapsed     | 240      |\n",
            "|    total_timesteps  | 143372   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.364    |\n",
            "|    n_updates        | 33342    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -55.9    |\n",
            "|    exploration_rate | 0.658    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1432     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 241      |\n",
            "|    total_timesteps  | 143872   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.87     |\n",
            "|    n_updates        | 33467    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -56.3    |\n",
            "|    exploration_rate | 0.657    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1436     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 242      |\n",
            "|    total_timesteps  | 144316   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.46     |\n",
            "|    n_updates        | 33578    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -56      |\n",
            "|    exploration_rate | 0.656    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1440     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 243      |\n",
            "|    total_timesteps  | 144754   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.846    |\n",
            "|    n_updates        | 33688    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -56.3    |\n",
            "|    exploration_rate | 0.655    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1444     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 244      |\n",
            "|    total_timesteps  | 145199   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.627    |\n",
            "|    n_updates        | 33799    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -57.5    |\n",
            "|    exploration_rate | 0.654    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1448     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 244      |\n",
            "|    total_timesteps  | 145630   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.812    |\n",
            "|    n_updates        | 33907    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 109      |\n",
            "|    ep_rew_mean      | -55.9    |\n",
            "|    exploration_rate | 0.653    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1452     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 245      |\n",
            "|    total_timesteps  | 146026   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.16     |\n",
            "|    n_updates        | 34006    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -54.1    |\n",
            "|    exploration_rate | 0.652    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1456     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 246      |\n",
            "|    total_timesteps  | 146475   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.69     |\n",
            "|    n_updates        | 34118    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -53.9    |\n",
            "|    exploration_rate | 0.651    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1460     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 246      |\n",
            "|    total_timesteps  | 146842   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.01     |\n",
            "|    n_updates        | 34210    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -53.9    |\n",
            "|    exploration_rate | 0.65     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1464     |\n",
            "|    fps              | 594      |\n",
            "|    time_elapsed     | 247      |\n",
            "|    total_timesteps  | 147279   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.433    |\n",
            "|    n_updates        | 34319    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -53.4    |\n",
            "|    exploration_rate | 0.649    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1468     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 248      |\n",
            "|    total_timesteps  | 147773   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.96     |\n",
            "|    n_updates        | 34443    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -52.5    |\n",
            "|    exploration_rate | 0.648    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1472     |\n",
            "|    fps              | 593      |\n",
            "|    time_elapsed     | 249      |\n",
            "|    total_timesteps  | 148168   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.639    |\n",
            "|    n_updates        | 34541    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -51.2    |\n",
            "|    exploration_rate | 0.647    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1476     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 250      |\n",
            "|    total_timesteps  | 148587   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.592    |\n",
            "|    n_updates        | 34646    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -51.5    |\n",
            "|    exploration_rate | 0.646    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1480     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 251      |\n",
            "|    total_timesteps  | 148983   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.661    |\n",
            "|    n_updates        | 34745    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -50.6    |\n",
            "|    exploration_rate | 0.645    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1484     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 252      |\n",
            "|    total_timesteps  | 149397   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.818    |\n",
            "|    n_updates        | 34849    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -51.4    |\n",
            "|    exploration_rate | 0.644    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1488     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 252      |\n",
            "|    total_timesteps  | 149845   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.28     |\n",
            "|    n_updates        | 34961    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 111      |\n",
            "|    ep_rew_mean      | -53.3    |\n",
            "|    exploration_rate | 0.643    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1492     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 253      |\n",
            "|    total_timesteps  | 150265   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.897    |\n",
            "|    n_updates        | 35066    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -53.1    |\n",
            "|    exploration_rate | 0.642    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1496     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 254      |\n",
            "|    total_timesteps  | 150624   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 5.02     |\n",
            "|    n_updates        | 35155    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -52.2    |\n",
            "|    exploration_rate | 0.641    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1500     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 254      |\n",
            "|    total_timesteps  | 151042   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.42     |\n",
            "|    n_updates        | 35260    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 110      |\n",
            "|    ep_rew_mean      | -52.5    |\n",
            "|    exploration_rate | 0.64     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1504     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 255      |\n",
            "|    total_timesteps  | 151474   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.482    |\n",
            "|    n_updates        | 35368    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -50.2    |\n",
            "|    exploration_rate | 0.639    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1508     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 256      |\n",
            "|    total_timesteps  | 151908   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.25     |\n",
            "|    n_updates        | 35476    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -50.9    |\n",
            "|    exploration_rate | 0.638    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1512     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 257      |\n",
            "|    total_timesteps  | 152374   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.57     |\n",
            "|    n_updates        | 35593    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -51.1    |\n",
            "|    exploration_rate | 0.637    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1516     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 257      |\n",
            "|    total_timesteps  | 152777   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.913    |\n",
            "|    n_updates        | 35694    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -51.4    |\n",
            "|    exploration_rate | 0.636    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1520     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 258      |\n",
            "|    total_timesteps  | 153110   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.986    |\n",
            "|    n_updates        | 35777    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -51.7    |\n",
            "|    exploration_rate | 0.635    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1524     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 259      |\n",
            "|    total_timesteps  | 153575   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.423    |\n",
            "|    n_updates        | 35893    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -52.4    |\n",
            "|    exploration_rate | 0.634    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1528     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 259      |\n",
            "|    total_timesteps  | 153987   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 35996    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -52.5    |\n",
            "|    exploration_rate | 0.633    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1532     |\n",
            "|    fps              | 592      |\n",
            "|    time_elapsed     | 260      |\n",
            "|    total_timesteps  | 154477   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.888    |\n",
            "|    n_updates        | 36119    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -53.2    |\n",
            "|    exploration_rate | 0.632    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1536     |\n",
            "|    fps              | 591      |\n",
            "|    time_elapsed     | 262      |\n",
            "|    total_timesteps  | 154954   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.83     |\n",
            "|    n_updates        | 36238    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 107      |\n",
            "|    ep_rew_mean      | -53.1    |\n",
            "|    exploration_rate | 0.631    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1540     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 263      |\n",
            "|    total_timesteps  | 155411   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.7      |\n",
            "|    n_updates        | 36352    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -53.9    |\n",
            "|    exploration_rate | 0.63     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1544     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 263      |\n",
            "|    total_timesteps  | 155829   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.363    |\n",
            "|    n_updates        | 36457    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -53.7    |\n",
            "|    exploration_rate | 0.629    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1548     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 264      |\n",
            "|    total_timesteps  | 156248   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.683    |\n",
            "|    n_updates        | 36561    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -55      |\n",
            "|    exploration_rate | 0.628    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1552     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 265      |\n",
            "|    total_timesteps  | 156668   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.38     |\n",
            "|    n_updates        | 36666    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -56.9    |\n",
            "|    exploration_rate | 0.627    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1556     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 265      |\n",
            "|    total_timesteps  | 156994   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.54     |\n",
            "|    n_updates        | 36748    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -56.2    |\n",
            "|    exploration_rate | 0.626    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1560     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 266      |\n",
            "|    total_timesteps  | 157443   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.442    |\n",
            "|    n_updates        | 36860    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -53.4    |\n",
            "|    exploration_rate | 0.625    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1564     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 267      |\n",
            "|    total_timesteps  | 157918   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.06     |\n",
            "|    n_updates        | 36979    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -53.5    |\n",
            "|    exploration_rate | 0.624    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1568     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 268      |\n",
            "|    total_timesteps  | 158365   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.416    |\n",
            "|    n_updates        | 37091    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -52.6    |\n",
            "|    exploration_rate | 0.623    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1572     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 268      |\n",
            "|    total_timesteps  | 158718   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.954    |\n",
            "|    n_updates        | 37179    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 105      |\n",
            "|    ep_rew_mean      | -53.4    |\n",
            "|    exploration_rate | 0.622    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1576     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 269      |\n",
            "|    total_timesteps  | 159039   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.831    |\n",
            "|    n_updates        | 37259    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -52.8    |\n",
            "|    exploration_rate | 0.621    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1580     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 270      |\n",
            "|    total_timesteps  | 159598   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 1.49     |\n",
            "|    n_updates        | 37399    |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 106      |\n",
            "|    ep_rew_mean      | -53.1    |\n",
            "|    exploration_rate | 0.62     |\n",
            "| time/               |          |\n",
            "|    episodes         | 1584     |\n",
            "|    fps              | 590      |\n",
            "|    time_elapsed     | 271      |\n",
            "|    total_timesteps  | 160045   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.559    |\n",
            "|    n_updates        | 37511    |\n",
            "----------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-b544245a62a8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# 训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2_000_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# 评估\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/dqn/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     ) -> SelfDQN:\n\u001b[0;32m--> 267\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    268\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;31m# Special case when the user passes `gradient_steps=0`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgradient_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgradient_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/dqn/dqn.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Update learning rate according to schedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m_update_learning_rate\u001b[0;34m(self, optimizers)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \"\"\"\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# Log the current learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train/learning_rate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_progress_remaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/logger.py\u001b[0m in \u001b[0;36mrecord\u001b[0;34m(self, key, value, exclude)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \"\"\"\n\u001b[1;32m    514\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_to_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_to_excluded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecord_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/logger.py\u001b[0m in \u001b[0;36mto_tuple\u001b[0;34m(string_or_tuple)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_formats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_formats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_or_tuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \"\"\"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2HaGRVEYGQS"
      },
      "source": [
        "## Testing\n",
        "The testing result will be the average reward of 5 testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yFuUKKRYH73"
      },
      "source": [
        "fix(env, seed)\n",
        "agent.network.eval()  # set the network into evaluation mode\n",
        "NUM_OF_TEST = 5 # Do not revise this !!!\n",
        "test_total_reward = []\n",
        "action_list = []\n",
        "for i in range(NUM_OF_TEST):\n",
        "  actions = []\n",
        "  state, _ = env.reset()\n",
        "\n",
        "  img = plt.imshow(env.render())\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "      action, _ = agent.sample(state)\n",
        "      actions.append(action)\n",
        "      # state, reward, done, _ = env.step(action)\n",
        "      state, reward, terminated, truncated, info = env.step(action)\n",
        "      done = terminated or truncated\n",
        "\n",
        "      total_reward += reward\n",
        "\n",
        "      img.set_data(env.render())\n",
        "      display.display(plt.gcf())\n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "  print(total_reward)\n",
        "  test_total_reward.append(total_reward)\n",
        "\n",
        "  action_list.append(actions) # save the result of testing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Actor Critic\n",
        "fix(env, seed)\n",
        "agent.actor.eval() # set the network into evaluation mode\n",
        "agent.critic.eval()\n",
        "NUM_OF_TEST = 5 # Do not revise this !!!\n",
        "test_total_reward = []\n",
        "action_list = []\n",
        "for i in range(NUM_OF_TEST):\n",
        "  actions = []\n",
        "  state, _ = env.reset()\n",
        "\n",
        "  img = plt.imshow(env.render())\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "      action, _, _ = agent.sample(state)\n",
        "      actions.append(action)\n",
        "      # state, reward, done, _ = env.step(action)\n",
        "      state, reward, terminated, truncated, info = env.step(action)\n",
        "      done = terminated or truncated\n",
        "\n",
        "      total_reward += reward\n",
        "\n",
        "      img.set_data(env.render())\n",
        "      display.display(plt.gcf())\n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "  print(total_reward)\n",
        "  test_total_reward.append(total_reward)\n",
        "\n",
        "  action_list.append(actions) # save the result of testing\n"
      ],
      "metadata": {
        "id": "Dr3ia4w813hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For DQN\n",
        "fix(env, seed)\n",
        "\n",
        "del model # remove to demonstrate saving and loading\n",
        "# 加载模型\n",
        "model = DQN.load(\"dqn_lunarlander_best\")\n",
        "\n",
        "NUM_OF_TEST = 5 # Do not revise this !!!\n",
        "test_total_reward = []\n",
        "action_list = []\n",
        "\n",
        "\n",
        "for i in range(NUM_OF_TEST):\n",
        "  actions = []\n",
        "  state, _ = env.reset()\n",
        "\n",
        "  img = plt.imshow(env.render())\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "      action, _states = model.predict(state, deterministic=True)\n",
        "      actions.append(action.item())\n",
        "      # state, reward, done, _ = env.step(action)\n",
        "      state, reward, terminated, truncated, info = env.step(action)\n",
        "      done = terminated or truncated\n",
        "\n",
        "      total_reward += reward\n",
        "\n",
        "      img.set_data(env.render())\n",
        "      display.display(plt.gcf())\n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "  print(total_reward)\n",
        "  test_total_reward.append(total_reward)\n",
        "\n",
        "  action_list.append(actions) # save the result of testing"
      ],
      "metadata": {
        "id": "JjLaD0A7l6iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aex7mcKr0J01"
      },
      "source": [
        "print(np.mean(test_total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leyebGYRpqsF"
      },
      "source": [
        "Action list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGAH4YWDpp4u"
      },
      "source": [
        "print(\"Action list looks like \", action_list)\n",
        "print(\"Action list's shape looks like \", np.shape(np.array(action_list, dtype=object)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNkmwucrHMen"
      },
      "source": [
        "Analysis of actions taken by agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHdAItjj1nxw"
      },
      "source": [
        "distribution = {}\n",
        "for actions in action_list:\n",
        "  for action in actions:\n",
        "    if action not in distribution.keys():\n",
        "      distribution[action] = 1\n",
        "    else:\n",
        "      distribution[action] += 1\n",
        "print(distribution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ricE0schY75M"
      },
      "source": [
        "Saving the result of Model Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZsMkGmIY42b"
      },
      "source": [
        "PATH = \"Action_List.npy\" # Can be modified into the name or path you want\n",
        "np.save(PATH ,np.array(action_list, dtype=object))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asK7WfbkaLjt"
      },
      "source": [
        "### This is the file you need to submit !!!\n",
        "Download the testing result to your device\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-CqyhHzaWAL"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seT4NUmWmAZ1"
      },
      "source": [
        "# Server\n",
        "The code below simulate the environment on the judge server. Can be used for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U69c-YTxaw6b"
      },
      "source": [
        "action_list = np.load(PATH,allow_pickle=True) # The action list you upload\n",
        "seed = 2023 # Do not revise this\n",
        "fix(env, seed)\n",
        "\n",
        "agent.network.eval()  # set network to evaluation mode\n",
        "\n",
        "test_total_reward = []\n",
        "if len(action_list) != 5:\n",
        "  print(\"Wrong format of file !!!\")\n",
        "  exit(0)\n",
        "for actions in action_list:\n",
        "  state, _ = env.reset()\n",
        "  img = plt.imshow(env.render())\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "\n",
        "  for action in actions:\n",
        "      state, reward, terminated, truncated, info = env.step(action)\n",
        "      done = terminated or truncated\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "  print(f\"Your reward is : %.2f\"%total_reward)\n",
        "  test_total_reward.append(total_reward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Actor Critic and DQN\n",
        "action_list = np.load(PATH,allow_pickle=True) # The action list you upload\n",
        "seed = 2023 # Do not revise this\n",
        "fix(env, seed)\n",
        "\n",
        "# agent.actor.eval()  # set network to evaluation mode\n",
        "# agent.critic.eval()  # set network to evaluation mode\n",
        "\n",
        "test_total_reward = []\n",
        "if len(action_list) != 5:\n",
        "  print(\"Wrong format of file !!!\")\n",
        "  exit(0)\n",
        "\n",
        "for actions in action_list:\n",
        "  state, _ = env.reset()\n",
        "  img = plt.imshow(env.render())\n",
        "\n",
        "  total_reward = 0\n",
        "\n",
        "  done = False\n",
        "\n",
        "  for action in actions:\n",
        "      state, reward, terminated, truncated, info = env.step(action)\n",
        "      done = terminated or truncated\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "  print(f\"Your reward is : %.2f\"%total_reward)\n",
        "  test_total_reward.append(total_reward)"
      ],
      "metadata": {
        "id": "EXI8nq1C2Sz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjFBWwQP1hVe"
      },
      "source": [
        "# Your score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpJpZz3Wbm0X"
      },
      "source": [
        "print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUBtYXG2eaqf"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Below are some useful tips for you to get high score.\n",
        "\n",
        "- [DRL Lecture 1: Policy Gradient (Review)](https://youtu.be/z95ZYgPgXOY)\n",
        "- [ML Lecture 23-3: Reinforcement Learning (including Q-learning) start at 30:00](https://youtu.be/2-JNBzCq77c?t=1800)\n",
        "- [Lecture 7: Policy Gradient, David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)\n"
      ]
    }
  ]
}